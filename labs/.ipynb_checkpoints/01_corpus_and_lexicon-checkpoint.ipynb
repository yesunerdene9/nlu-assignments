{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Corpus and Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives\n",
    "- Understanding: \n",
    "    - relation between corpus and lexicon\n",
    "    - effects of pre-processing (tokenization) on lexicon\n",
    "    \n",
    "- Learning how to:\n",
    "    - load basic corpora for processing\n",
    "    - compute basic descriptive statistic of a corpus\n",
    "    - building lexicon and frequency lists from a corpus\n",
    "    - perform basic lexicon operations\n",
    "    - perform basic text pre-processing (tokenization and sentence segmentation) using python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 2: Regular Expressions, Text Normalization, Edit Distance](https://web.stanford.edu/~jurafsky/slp3/2.pdf) \n",
    "- NLTK \n",
    "    - [Chapter 2: Accessing Text Corpora and Lexical Resources](https://www.nltk.org/book/ch02.html)\n",
    "    - [Chapter 3: Processing Raw Text](https://www.nltk.org/book/ch03.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Requirements (Not required if you have installed the given env)\n",
    "\n",
    "- [NLTK](http://www.nltk.org/)\n",
    "    - run `pip install nltk`\n",
    "    \n",
    "- [spaCy](https://spacy.io/)\n",
    "    - run `pip install spacy`\n",
    "    - run `python -m spacy download en_core_web_sm` to install English language model (`spacy>=3.0`)\n",
    "\n",
    "- [scikit-learn](https://scikit-learn.org/)\n",
    "    - run `pip install scikit-learn`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Python Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this, we briefly see the basic data structures in Python language. However, this do not substitute the suggested Python guides. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Lists\n",
    "Lists are one of the four (i.e. Dictionaries, Tuples, Sets) of built-in data structures. They can store multiple items of any type (e.g. objects, functions, strings, integers, etc.). To declare a list we use the squared brackets `[]`. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green\n",
      "blue\n",
      "yellow\n",
      "The length is 3\n",
      "-----------------------------------------------------------------------------------------\n",
      "green\n",
      "blue\n",
      "yellow\n",
      "42\n",
      "7\n",
      "3\n",
      "128\n",
      "The length is 7\n"
     ]
    }
   ],
   "source": [
    "colors = ['green', 'blue', 'yellow']\n",
    "random_numbers = [42, 7, 3, 128]\n",
    "for c in colors:\n",
    "    print(c)\n",
    "print('The length is', len(colors))\n",
    "print('-'*89)\n",
    "# Arrays can contain anything\n",
    "for n in random_numbers:\n",
    "    colors.append(n)\n",
    "    \n",
    "for elem in colors:\n",
    "    print(elem)\n",
    "print('The length is', len(colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In python assignments are done by call by reference or call by value. To better understand this important aspect of python check [this](https://www.geeksforgeeks.org/is-python-call-by-reference-or-call-by-value/) out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colors: ['green', 'blue', 'yellow', 'ALPHA']\n",
      "Tmp colors: ['green', 'blue', 'yellow', 'ALPHA']\n"
     ]
    }
   ],
   "source": [
    "# Here assignments are done by reference, called \"Call by Object Reference\"\n",
    "colors = ['green', 'blue', 'yellow']\n",
    "tmp_colors = colors\n",
    "tmp_colors.append('ALPHA')\n",
    "print('Colors:', colors)\n",
    "print('Tmp colors:', tmp_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['green', 'blue', 'yellow']\n",
    "tmp_colors = []\n",
    "for c in colors:\n",
    "    tmp_colors.append(c)\n",
    "    \n",
    "tmp_colors.append('ALPHA')\n",
    "print('Colors:', colors)\n",
    "print('Tmp colors:', tmp_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['green', 'blue', 'yellow', 'ALPHA']\n",
      "['blue']\n"
     ]
    }
   ],
   "source": [
    "# This is called list of comprehension\n",
    "# It's basically a compact for loop with append\n",
    "# It's mainly used for copying arrays or filtering\n",
    "tmp = [c for c in colors]\n",
    "print(tmp)\n",
    "tmp = [c for c in colors if c == \"blue\"]\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Dictionaries\n",
    "Dictionaries store information in the format key: value pair. The keys are unique, duplicates are not allowed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: Italy Capital: Rome\n",
      "Country: U.S.A Capital: Washington D.C.\n",
      "Country: Japan Capital: Tokyo\n",
      "Country: Asaland Capital: Asgard\n",
      "Country: Galactic Empire Capital: Coruscant\n",
      "♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦\n",
      "Countries: ['Italy', 'U.S.A', 'Japan', 'Asaland', 'Galactic Empire']\n",
      "Countries: ['Italy', 'U.S.A', 'Japan', 'Asaland', 'Galactic Empire']\n",
      "♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦\n",
      "Capitals: ['Rome', 'Washington D.C.', 'Tokyo', 'Asgard', 'Coruscant']\n"
     ]
    }
   ],
   "source": [
    "capitals = {\"Italy\":\"Rome\", \"U.S.A\": \"Washington D.C.\",  \"Japan\": \"Tokyo\", \"Asaland\": \"Asgard\", \"Galactic Empire\": \"Coruscant\"}\n",
    "for key, value in capitals.items():\n",
    "    print(\"Country:\", key, \"Capital:\", value)\n",
    "print(\"♦\"*89)\n",
    "countries = [country for country in capitals]\n",
    "print(\"Countries:\", countries)\n",
    "# OR\n",
    "countries = [country for country in capitals.keys()]\n",
    "print(\"Countries:\", countries)\n",
    "print(\"♦\"*89)\n",
    "capitals = [country for country in capitals.values()]\n",
    "print(\"Capitals:\", capitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Corpora and Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Corpus\n",
    "\n",
    "[Corpus](https://en.wikipedia.org/wiki/Text_corpus) is a collection of written or spoken texts that is used for language research. Before doing anything with a corpus we need to know its properties:\n",
    "\n",
    "__Corpus Properties__:\n",
    "- *Format* -- how to read/load it?\n",
    "- *Natural Language* -- which tools/models can I use?\n",
    "- *Annotation* -- what it is intended for?\n",
    "- *Split* for __Evaluation__: (terminology varies from source to source)\n",
    "\n",
    "| Set         | Purpose                                       |\n",
    "|:------------|:----------------------------------------------|\n",
    "| Training    | training model, extracting rules, etc.        |\n",
    "| Development | tuning, optimization, intermediate evaluation |\n",
    "| Test        | final evaluation (remains unseen)             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1. Text Corpora in NLTK\n",
    "NLTK provides several corpora with loading functions. Plain text corpora come from a _Project Gutenberg_.\n",
    "\n",
    "`nltk.corpus.gutenberg.fileids()` lists available books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/yesunerdenejargalsaikhan/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yesunerdenejargalsaikhan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # developing your own model good for this lib\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. Units of Text Corpus\n",
    "Depending on a goal, corpus can be seen as a sequence of:\n",
    "- characters\n",
    "- words (tokens)\n",
    "- sentences\n",
    "- paragraphs\n",
    "- document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each level, in turn, can be seen as a sequence of elements of the previous level.\n",
    "\n",
    "- word -- a sequence of characters\n",
    "- sentence -- a sequence of words\n",
    "- paragraph -- a sequence of sentences\n",
    "- document -- a sequence of paragraphs (or sentences, depending on our purpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3. Loading NLTK Corpora\n",
    "\n",
    "NLTK provides functions to load a corpus using these different levels, as `raw` (characters), `words`, and `sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: [Alice's A\n",
      "words: ['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll']\n",
      "sents: [['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']'], ['CHAPTER', 'I', '.'], ['Down', 'the', 'Rabbit', '-', 'Hole'], ['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\", 'thought', 'Alice', \"'\", 'without', 'pictures', 'or', 'conversation', \"?'\"], ['So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', '(', 'as', 'well', 'as', 'she', 'could', ',', 'for', 'the', 'hot', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and', 'stupid', '),', 'whether', 'the', 'pleasure', 'of', 'making', 'a', 'daisy', '-', 'chain', 'would', 'be', 'worth', 'the', 'trouble', 'of', 'getting', 'up', 'and', 'picking', 'the', 'daisies', ',', 'when', 'suddenly', 'a', 'White', 'Rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her', '.'], ['There', 'was', 'nothing', 'so', 'VERY', 'remarkable', 'in', 'that', ';', 'nor', 'did', 'Alice', 'think', 'it', 'so', 'VERY', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'Rabbit', 'say', 'to', 'itself', ',', \"'\", 'Oh', 'dear', '!'], ['Oh', 'dear', '!'], ['I', 'shall', 'be', 'late', \"!'\"], ['(', 'when', 'she', 'thought', 'it', 'over', 'afterwards', ',', 'it', 'occurred', 'to', 'her', 'that', 'she', 'ought', 'to', 'have', 'wondered', 'at', 'this', ',', 'but', 'at', 'the', 'time', 'it', 'all', 'seemed', 'quite', 'natural', ');', 'but', 'when', 'the', 'Rabbit', 'actually', 'TOOK', 'A', 'WATCH', 'OUT', 'OF', 'ITS', 'WAISTCOAT', '-', 'POCKET', ',', 'and', 'looked', 'at', 'it', ',', 'and', 'then', 'hurried', 'on', ',', 'Alice', 'started', 'to', 'her', 'feet', ',', 'for', 'it', 'flashed', 'across', 'her', 'mind', 'that', 'she', 'had', 'never', 'before', 'seen', 'a', 'rabbit', 'with', 'either', 'a', 'waistcoat', '-', 'pocket', ',', 'or', 'a', 'watch', 'to', 'take', 'out', 'of', 'it', ',', 'and', 'burning', 'with', 'curiosity', ',', 'she', 'ran', 'across', 'the', 'field', 'after', 'it', ',', 'and', 'fortunately', 'was', 'just', 'in', 'time', 'to', 'see', 'it', 'pop', 'down', 'a', 'large', 'rabbit', '-', 'hole', 'under', 'the', 'hedge', '.'], ['In', 'another', 'moment', 'down', 'went', 'Alice', 'after', 'it', ',', 'never', 'once', 'considering', 'how', 'in', 'the', 'world', 'she', 'was', 'to', 'get', 'out', 'again', '.']]\n"
     ]
    }
   ],
   "source": [
    "alice_chars = nltk.corpus.gutenberg.raw('carroll-alice.txt')\n",
    "print('chars:', alice_chars[0:10])\n",
    "alice_words = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
    "print('words:', alice_words[0:10])\n",
    "alice_sents = nltk.corpus.gutenberg.sents('carroll-alice.txt')\n",
    "print('sents:', alice_sents[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. Corpus Descriptive Statistics (Counting)\n",
    "\n",
    "*Corpus* can be described in terms of:\n",
    "\n",
    "- total number of characters\n",
    "- total number of words (_tokens_: includes punctuation, etc.)\n",
    "- total number of sentences\n",
    "\n",
    "- minimum/maximum/average number of character per token\n",
    "- minimum/maximum/average number of words per sentence\n",
    "- minimum/maximum/average number of sentences per document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Example__\n",
    "\n",
    "$$\\text{Av. Token Count} = \\frac{\\text{count}(tokens)}{\\text{count}(sentences)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's compute average sentence length & round to the closest integer\n",
    "round(len(alice_words)/len(alice_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG sent len 20\n",
      "MIN sent len 2\n",
      "MAX sent len 204\n"
     ]
    }
   ],
   "source": [
    "# let's compute length of each sentence\n",
    "sent_lens = [len(sent) for sent in alice_sents]\n",
    "# let's compute length of each word\n",
    "word_lens = [len(word) for word in alice_words]\n",
    "# let's compute length the number of characters in each sentence\n",
    "chars_lens = [len(''.join(sent)) for sent in alice_sents]\n",
    "\n",
    "avg_sent_len = round(sum(sent_lens)/len(sent_lens))\n",
    "min_sent_len = min(sent_lens)\n",
    "max_sent_len = max(sent_lens)\n",
    "print(\"AVG sent len\", avg_sent_len)\n",
    "print(\"MIN sent len\", min_sent_len)\n",
    "print(\"MAX sent len\", max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H e l l o\n",
      "H⭐e⭐l⭐l⭐o\n",
      "['H⭐e⭐l⭐l⭐o']\n"
     ]
    }
   ],
   "source": [
    "# JOIN built-in function example\n",
    "tmp = ['H', 'e', 'l', 'l', 'o']\n",
    "print(' '.join(tmp))\n",
    "print('⭐'.join(tmp))\n",
    "print('⭐'.join(tmp).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 1\n",
    "\n",
    "- Define a function to compute corpus descriptive statistics\n",
    "\n",
    "    - input:\n",
    "        - raw text (Chars)\n",
    "        - words\n",
    "        - sentences\n",
    "    - output (print): \n",
    "        - average number of:\n",
    "            - chars per word\n",
    "            - words per sentence\n",
    "            - chars per sentence\n",
    "        - Size of the longest word and sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word per sentence 20\n",
      "Char per word\n",
      "Char per sentence\n",
      "Longest sentence\n",
      "Longest word\n"
     ]
    }
   ],
   "source": [
    "def statistics(words, sents):\n",
    "    word_lens = [len(word) for word in words]# Add word lens\n",
    "    sent_lens = [len(sent) for sent in sents] # Add sentence lens\n",
    "    chars_in_sents = [len(''.join(sent)) for sent in sents] # Add char lens\n",
    "    \n",
    "    word_per_sent = round(sum(sent_lens) / len(sents))\n",
    "    char_per_word = round(sum(word_lens) / len(words))\n",
    "    char_per_sent = round(sum(chars_in_sents) / len(sents))\n",
    "    \n",
    "    longest_sentence = ''#max() # max(...)\n",
    "    longest_word =  ''#max(word_lens) # max(...)\n",
    "    \n",
    "    return word_per_sent, char_per_word, char_per_sent, longest_sentence, longest_word\n",
    "\n",
    "word_per_sent, char_per_word, char_per_sent, longest_sent, longest_word = statistics(alice_words, alice_sents)\n",
    "\n",
    "print('Word per sentence', word_per_sent)\n",
    "print('Char per word', )\n",
    "print('Char per sentence', )\n",
    "print('Longest sentence', )\n",
    "print('Longest word', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Lexicon\n",
    "\n",
    "[Lexicon](https://en.wikipedia.org/wiki/Lexicon) is the *vocabulary* of a language. In linguistics, a lexicon is a language's inventory of lexemes.\n",
    "\n",
    "Linguistic theories generally regard human languages as consisting of two parts: a lexicon, essentially a catalog of a language's words; and a grammar, a system of rules which allow for the combination of those words into meaningful sentences. \n",
    "\n",
    "*Lexicon (or Vocabulary) Size* is one of the statistics reported for corpora. While *Word Count* is the number of __tokens__, *Lexicon Size* is the number of __types__ (unique words).\n",
    "\n",
    "#### Token vs Word\n",
    "The ***tokens*** are the elements in a sentences and they are used to compute the **occurrences** of a word. Instead, ***words*** are the **unique** elements that compose the Lexicon or Vocabulary of a corpus. We can think of words as classes and tokens as instances of those classes.\n",
    "\n",
    "<br>\n",
    "\n",
    "**For example**:\n",
    "<br> \n",
    "-   How many tokens are in the sentence ***to be, or not to be***? \n",
    "    - Answer: ?\n",
    "\n",
    "-   How many words?\n",
    "    -   Answer: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Lexicon and Its Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Constructing Lexicon and Computing its Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since lexicon is a list of unique elements, it is a `set` of corpus words (i.e. tokens).\n",
    "Consequently, its size is the size of the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3016"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_lexicon = set(alice_words)\n",
    "len(alice_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__:\n",
    "We did not process our corpus in any way. Consequently, words with case variations are different entries in our lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('ALL' in alice_lexicon)\n",
    "print('All' in alice_lexicon)\n",
    "print('all' in alice_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Lowercased Lexicon\n",
    "Let's lowercase our corpus and re-compute the lexicon size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2636"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_lexicon = set([w.lower() for w in alice_words])\n",
    "len(alice_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('ALL' in alice_lexicon)\n",
    "print('All' in alice_lexicon)\n",
    "print('all' in alice_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Frequency List\n",
    "\n",
    "In Natural Language Processing (NLP), [a frequency list](https://en.wikipedia.org/wiki/Word_lists_by_frequency) is a sorted list of words (word types) together with their frequency, where frequency here usually means the number of occurrences in a given corpus, from which the rank can be derived as the position in the list.\n",
    "\n",
    "What is a \"word\"?\n",
    "\n",
    "- case sensitive counts\n",
    "- case insensitive counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Computing Frequency List with python\n",
    "\n",
    "In python, frequency list can be constructed in several ways. The most convenient is the `Counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({',': 1993, \"'\": 1731, 'the': 1527, 'and': 802, '.': 764, 'to': 725, 'a': 615, 'I': 543, 'it': 527, 'she': 509, 'of': 500, 'said': 456, \",'\": 397, 'Alice': 396, 'in': 357, 'was': 352, 'you': 345, \"!'\": 278, 'that': 275, 'as': 246, 'her': 243, ':': 216, 't': 216, 'at': 202, 's': 195, 'on': 189, \".'\": 187, ';': 186, 'had': 177, 'with': 175, 'all': 173, '!': 155, \"?'\": 154, 'be': 145, '-': 141, 'for': 140, '--': 140, 'but': 133, 'not': 129, 'they': 129, 'very': 126, 'little': 125, 'so': 124, 'out': 116, 'this': 113, 'The': 108, 'he': 101, 'down': 99, 'up': 98, 'is': 97, 'about': 94, 'one': 94, 'his': 94, 'what': 93, 'them': 88, 'know': 87, 'were': 85, 'like': 84, 'went': 83, 'again': 83, 'herself': 83, 'if': 78, 'or': 76, 'thought': 74, 'Queen': 74, 'could': 73, 'have': 73, 'then': 72, 'would': 70, 'no': 69, 'when': 69, 'do': 68, 'time': 68, 'into': 67, 'And': 67, 'see': 66, 'there': 65, 'It': 64, 'off': 62, 'me': 61, 'King': 61, 'did': 60, '*': 60, 'Turtle': 59, 'began': 58, 'm': 58, 'can': 57, 'way': 56, 'll': 56, 'its': 56, 'Mock': 56, 'by': 55, 'my': 55, 'Hatter': 55, 'Gryphon': 55, 'quite': 53, 'your': 53, 'an': 52, 'much': 51, 'say': 51, \"--'\": 51, 'don': 51, 'You': 51, '(': 50, 'think': 50, 'their': 50, 'head': 50, 'thing': 49, 'some': 48, '\"': 48, 'who': 48, 'now': 48, 'go': 47, 'only': 47, 'more': 47, 'voice': 47, 'Rabbit': 45, 'looked': 45, 'just': 45, 'got': 45, 'get': 44, 'are': 44, 'first': 44, 've': 43, 'never': 42, 'What': 42, 'Duchess': 42, 'must': 41, 'which': 41, 'round': 41, 'well': 40, 'over': 40, 'how': 40, 'such': 40, 'came': 40, 'other': 40, 'tone': 40, 'Dormouse': 40, 'here': 39, 'She': 39, 'great': 39, 'any': 39, 'him': 39, 're': 38, 'been': 38, 'back': 38, 'after': 37, 'But': 37, 'before': 36, 'Oh': 35, '?': 35, 'from': 34, 'March': 34, 'There': 33, 'large': 33, 'two': 32, 'looking': 32, 'last': 32, 'once': 31, 'moment': 31, 'put': 31, 'things': 31, 'long': 31, 'Hare': 31, 'nothing': 30, 'made': 30, 'found': 30, 'right': 30, 'door': 30, 'heard': 30, 'Mouse': 30, 'day': 29, 'eyes': 29, 'replied': 29, 'dear': 28, 'look': 28, 'next': 28, 'might': 28, 'three': 28, 'So': 27, 'seemed': 27, 'going': 27, 'make': 27, 'How': 27, 'Why': 27, 'should': 27, 'tell': 27, 'That': 27, 'd': 27, 'Caterpillar': 27, 'without': 26, 'upon': 26, 'good': 26, 'won': 26, 'course': 26, 'Cat': 26, 'too': 25, 'come': 25, 'rather': 25, 'away': 25, 'poor': 25, 'soon': 24, 'will': 24, 'shall': 23, 'took': 23, 'Well': 23, 'felt': 23, 'than': 23, 'same': 23, 'we': 23, 'added': 23, 'getting': 22, 'White': 22, 'another': 22, 'half': 22, 'jury': 22, 'words': 21, 'wish': 21, 'find': 21, 'Come': 21, 'minute': 21, 'They': 21, 'till': 21, 'yet': 21, 'Then': 21, 'sort': 20, 'No': 20, 'hand': 20, 'ever': 20, 'while': 20, 'cried': 20, 'sure': 20, 'He': 20, 'feet': 19, 'tried': 19, 'anything': 19, 'curious': 19, 'being': 19, 'use': 18, 'take': 18, 'wonder': 18, 'house': 18, \"'--\": 18, 'tea': 18, 'enough': 18, 'A': 17, 'even': 17, 'end': 17, 'spoke': 17, 'eat': 17, 'question': 17, 'side': 17, 'table': 17, 'sat': 17, 'As': 17, 'something': 17, 'old': 17, 'This': 17, 'asked': 17, 'court': 17, 'ran': 16, 'under': 16, '.)': 16, 'am': 16, 'talking': 16, 'bit': 16, 'turned': 16, 'garden': 16, 'hastily': 16, 'If': 16, 'doesn': 16, 'Bill': 16, 'seen': 15, 'idea': 15, 'air': 15, 'saying': 15, 'low': 15, 'high': 15, 'indeed': 15, 'face': 15, 'Who': 15, ':--': 15, 'done': 15, 'YOU': 15, 'called': 15, 'arm': 15, 'beginning': 14, 'hear': 14, 'itself': 14, 'ought': 14, 'saw': 14, 'near': 14, 'didn': 14, 'perhaps': 14, 'Dinah': 14, 'remember': 14, 'mouse': 14, 'trying': 14, 'because': 14, 'left': 14, 'anxiously': 14, 'set': 14, 'knew': 14, 'sea': 14, 'talk': 14, 'us': 14, 'better': 14, 'both': 14, 'baby': 14, 'mad': 14, 'close': 13, 'VERY': 13, 'people': 13, 'through': 13, 'Do': 13, 'cats': 13, 'behind': 13, 'However': 13, 'For': 13, 'certainly': 13, 'THAT': 13, 'size': 13, 'gave': 13, 'grow': 13, 'speak': 13, 'far': 13, 'kept': 13, 'please': 13, 'used': 13, 'change': 13, 'why': 13, 'Dodo': 13, 'whole': 13, '--\"': 13, 'room': 13, 'gone': 13, 'cook': 13, 'Soup': 13, 'dance': 13, 'CHAPTER': 12, 'suddenly': 12, 'wouldn': 12, 'many': 12, 'still': 12, 'among': 12, 'else': 12, 'afraid': 12, 'Now': 12, 'every': 12, 'begin': 12, 'finished': 12, 'game': 12, 'life': 12, 'deal': 12, 'queer': 12, 'everything': 12, 'try': 12, 'hands': 12, 'suppose': 12, 'always': 12, 'turning': 12, 'Yes': 12, 'Majesty': 12, 'book': 11, 'whether': 11, 'hurried': 11, 'In': 11, 'Let': 11, 'glad': 11, 'ask': 11, 'cat': 11, 'really': 11, 'hurry': 11, 'waited': 11, 'minutes': 11, 'child': 11, 'hardly': 11, 'makes': 11, 'growing': 11, 'gloves': 11, 'let': 11, 'these': 11, 'silence': 11, 'Of': 11, 'wasn': 11, 'Here': 11, 'may': 11, 'Pigeon': 11, 'Footman': 11, 'Off': 11, 'sitting': 10, 'having': 10, 'conversation': 10, 'own': 10, 'lessons': 10, 'though': 10, 'word': 10, 'name': 10, 'sight': 10, 'walked': 10, 'glass': 10, 'small': 10, 'opened': 10, 'bottle': 10, 'read': 10, 'children': 10, 'best': 10, 'tears': 10, 'box': 10, 'yourself': 10, 'pool': 10, 'fan': 10, 'thinking': 10, 'oh': 10, 'nearly': 10, 'offended': 10, 'We': 10, 'When': 10, 'creatures': 10, 'rest': 10, 'trial': 10, 'mean': 10, 'mouth': 10, 'where': 10, 'repeated': 10, 'remarked': 10, 'remark': 10, 'soldiers': 10, 'witness': 10, 'sister': 9, 'mind': 9, 'either': 9, 'heads': 9, 'couldn': 9, 'answer': 9, 'matter': 9, 'hall': 9, 'key': 9, 'rate': 9, 'those': 9, 'few': 9, ',)': 9, 'against': 9, 'want': 9, 'give': 9, 'help': 9, 'different': 9, 'tail': 9, 'foot': 9, 'Not': 9, 'Don': 9, 'birds': 9, 'party': 9, 'At': 9, 'continued': 9, 'believe': 9, 'angrily': 9, 'shook': 9, 'least': 9, 'reason': 9, 'together': 9, 'shouted': 9, 'keep': 9, 'timidly': 9, 'puzzled': 9, 'pig': 9, 'butter': 9, 'interrupted': 9, 'Knave': 9, 'join': 9, 'feel': 8, '),': 8, 'making': 8, 'slowly': 8, 'happen': 8, 'coming': 8, 'noticed': 8, 'top': 8, 'Which': 8, 'Would': 8, 'four': 8, 'opportunity': 8, ')': 8, 'distance': 8, 'seem': 8, 'dry': 8, 'bright': 8, 'waiting': 8, 'fact': 8, 'followed': 8, 'croquet': 8, 'lying': 8, 'work': 8, 'white': 8, 'ready': 8, 'hard': 8, 'changed': 8, 'live': 8, 'play': 8, 'eagerly': 8, 'meaning': 8, 'IS': 8, 'explain': 8, 'running': 8, 'window': 8, 'place': 8, 'appeared': 8, 'wood': 8, 'mushroom': 8, 'haven': 8, 'turn': 8, 'most': 8, 'nose': 8, 'tree': 8, 'beautiful': 8, 'asleep': 8, 'gardeners': 8, 'moral': 8, 'whiting': 8, 'slates': 8, 'tired': 7, 'hot': 7, 'watch': 7, 'world': 7, 'deep': 7, 'First': 7, 'fall': 7, 'listen': 7, 'fancy': 7, 'manage': 7, 'begun': 7, 'dream': 7, 'middle': 7, 'wondering': 7, 'golden': 7, 'open': 7, 'larger': 7, 'happened': 7, 'neck': 7, 'ME': 7, 'NOT': 7, 'feeling': 7, 'however': 7, 'leave': 7, 'generally': 7, 'eye': 7, 'surprised': 7, 'kind': 7, 'THE': 7, 'Just': 7, 'hair': 7, 'goes': 7, 'mine': 7, 'learn': 7, 'frightened': 7, 'chin': 7, 'hadn': 7, 'history': 7, 'beg': 7, 'our': 7, 'Lory': 7, 'important': 7, '.\"\\'': 7, 'story': 7, 'others': 7, 'YOUR': 7, 'stood': 7, 'grown': 7, 'business': 7, 'trees': 7, 'puppy': 7, 'each': 7, 'isn': 7, 'draw': 7, 'silent': 7, 'ground': 7, 'Cheshire': 7, 'pleased': 7, 'song': 7, 'bread': 7, 'Five': 7, 'hedgehog': 7, 'lobsters': 7, 'Soo': 7, 'oop': 7, 'trouble': 6, 'late': 6, 'pocket': 6, 'fell': 6, 'nice': 6, 'written': 6, 'leaves': 6, 'jumped': 6, 'roof': 6, 'inches': 6, 'along': 6, 'marked': 6, 'hold': 6, 'forgotten': 6, 'almost': 6, 'After': 6, 'English': 6, 'shoes': 6, 'myself': 6, 'S': 6, 'nonsense': 6, 'himself': 6, 'sir': 6, 'times': 6, 'repeat': 6, 'grin': 6, '!\"': 6, 'understand': 6, 'William': 6, 'pardon': 6, 'trembling': 6, 'subject': 6, 'fetch': 6, 'sit': 6, 'politely': 6, 'WHAT': 6, 'melancholy': 6, 'One': 6, 'liked': 6, 'has': 6, 'piece': 6, 'executed': 6, 'ARE': 6, 'does': 6, 'broken': 6, 'chimney': 6, 'full': 6, 'Will': 6, 'loud': 6, 'sharp': 6, 'Lizard': 6, 'guinea': 6, 'pigs': 6, 'arms': 6, 'youth': 6, 'serpent': 6, 'sleep': 6, 'fish': 6, 'sneezing': 6, 'told': 6, 'pepper': 6, 'ear': 6, 'dreadfully': 6, 'writing': 6, 'Two': 6, 'Hearts': 6, 'exclaimed': 6, 'executioner': 6, 'school': 6, 'e': 6, 'tarts': 6, 'evidence': 6, 'twice': 5, 'sleepy': 5, 'stupid': 5, 'across': 5, 'rabbit': 5, 'curiosity': 5, 'passed': 5, 'somebody': 5, 'home': 5, 'likely': 5, 'aloud': 5, 'walk': 5, 'Please': 5, 'asking': 5, 'night': 5, 'sometimes': 5, 'walking': 5, 'ears': 5, 'sadly': 5, 'shut': 5, \"('\": 5, \"';\": 5, 'WOULD': 5, 'simple': 5, 'cut': 5, 'finger': 5, 'ten': 5, 'nervous': 5, 'altogether': 5, 'remembered': 5, 'happens': 5, 'shan': 5, 'pair': 5, 'nine': 5, 'until': 5, 'kid': 5, 'dropped': 5, 'usual': 5, 'morning': 5, 'Ah': 5, 'SHE': 5, 'wrong': 5, 'sounded': 5, 'strange': 5, 'seems': 5, 'stay': 5, 'Tell': 5, 'sudden': 5, 'water': 5, 'case': 5, 'swam': 5, 'nearer': 5, 'speaking': 5, 'sentence': 5, 'shrill': 5, 'angry': 5, 'crowded': 5, 'means': 5, 'notice': 5, 'race': 5, 'exactly': 5, 'dare': 5, 'confusion': 5, 'call': 5, 'reply': 5, 'finish': 5, 'impatiently': 5, 'sighed': 5, 'temper': 5, 'young': 5, 'nobody': 5, 'moved': 5, 'Where': 5, 'Very': 5, 'doing': 5, 'direction': 5, 'stop': 5, 'interesting': 5, 'become': 5, 'write': 5, 'shouldn': 5, 'taking': 5, 'shriek': 5, 'An': 5, 'drew': 5, 'sky': 5, 'instantly': 5, 'surprise': 5, 'height': 5, 'quietly': 5, 'hookah': 5, 'man': 5, 'stand': 5, 'often': 5, 'eggs': 5, 'between': 5, 'While': 5, 'everybody': 5, 'meant': 5, 'waving': 5, 'All': 5, 'clock': 5, 'sing': 5, 'twinkle': 5, 'treacle': 5, 'M': 5, 'Seven': 5, 'faces': 5, 'procession': 5, 'pack': 5, 'knee': 5, 'whispered': 5, 'flamingo': 5, 'Lobster': 5, 'Beautiful': 5, 'evening': 5, 'pictures': 4, 'worth': 4, 'natural': 4, 'hole': 4, 'sides': 4, 'fear': 4, 'managed': 4, 'fallen': 4, 'earth': 4, 'several': 4, 'WAS': 4, 'sound': 4, 'girl': 4, 'mice': 4, 'bats': 4, 'passage': 4, 'corner': 4, 'except': 4, 'tiny': 4, 'second': 4, 'led': 4, 'shoulders': 4, 'paper': 4, 'taught': 4, 'deeply': 4, 'drink': 4, 'ventured': 4, 'reach': 4, 'sharply': 4, 'severely': 4, 'fond': 4, 'ONE': 4, 'person': 4, 'care': 4, ').': 4, 'new': 4, 'savage': 4, 'violently': 4, 'age': 4, 'Mabel': 4, 'puzzling': 4, 'twelve': 4, 'capital': 4, '--\"\\'': 4, 'On': 4, 'With': 4, '\"--': 4, 'alone': 4, 'CAN': 4, 'shrinking': 4, 'escape': 4, 'number': 4, 'French': 4, 'fire': 4, 'paws': 4, 'Are': 4, 'says': 4, 'pale': 4, 'shore': 4, 'animals': 4, 'uncomfortable': 4, 'argument': 4, 'wanted': 4, 'frowning': 4, 'IT': 4, 'solemnly': 4, 'pointing': 4, 'confused': 4, 'thimble': 4, 'short': 4, 'bowed': 4, '.\"': 4, 'Said': 4, 'judge': 4, 'breath': 4, ',\"': 4, 'Hold': 4, 'tongue': 4, 'particular': 4, 'since': 4, 'vanished': 4, 'Mary': 4, 'Ann': 4, '\\'\"': 4, 'By': 4, 'chance': 4, 'ordered': 4, 'answered': 4, 'Sure': 4, 'yer': 4, 'honour': 4, 'THIS': 4, 'COULD': 4, 'hearing': 4, '?--': 4, 'slate': 4, 'master': 4, 'fellow': 4, 'dead': 4, 'doubt': 4, 'lay': 4, 'crowd': 4, 'held': 4, 'plan': 4, 'difficulty': 4, 'stick': 4, 'grass': 4, ',\"\\'': 4, 'perfectly': 4, 'none': 4, 'questions': 4, 'decidedly': 4, 'thoughtfully': 4, 'green': 4, 'HAVE': 4, 'screamed': 4, 'indignantly': 4, 'taken': 4, 'lives': 4, 'otherwise': 4, 'dish': 4, 'days': 4, 'kitchen': 4, 'soup': 4, 'jumping': 4, 'carried': 4, 'hours': 4, 'busily': 4, 'beat': 4, 'wow': 4, 'verse': 4, 'creature': 4, 'grunted': 4, 'Call': 4, 'fast': 4, 'crumbs': 4, 'shoulder': 4, 'twinkling': 4, 'HE': 4, 'sigh': 4, 'bottom': 4, 'Take': 4, 'begins': 4, 'rose': 4, 'rule': 4, 'arches': 4, 'players': 4, 'ALL': 4, 'Never': 4, 'hers': 4, 'Quadrille': 4, 'sobs': 4, 'porpoise': 4, 'mouths': 4, 'adventures': 4, 'Beau': 4, 'ootiful': 4, 'jurors': 4, 'verdict': 4, 'officers': 4, 'suppressed': 4, 'jurymen': 4, 'Nothing': 4, 'verses': 4, '[': 3, 'Adventures': 3, 'Wonderland': 3, 'Down': 3, 'bank': 3, 'peeped': 3, 'reading': 3, 'considering': 3, 'OF': 3, 'dark': 3, 'filled': 3, 'past': 3, 'stairs': 3, 'true': 3, 'miles': 3, 'knowledge': 3, 'grand': 3, 'funny': 3, 'listening': 3, 'hope': 3, 'catch': 3, 'bat': 3, 'hurt': 3, 'lost': 3, 'whiskers': 3, 'longer': 3, 'hanging': 3, 'alas': 3, 'delight': 3, 'telescope': 3, 'impossible': 3, 'hoping': 3, 'rules': 3, 'poison': 3, 'knife': 3, 'certain': 3, 'later': 3, 'finding': 3, 'further': 3, 'candle': 3, 'decided': 3, 'possibly': 3, 'legs': 3, 'cake': 3, 'smaller': 3, 'holding': 3, 'remained': 3, 'expecting': 3, 'dull': 3, 'opening': 3, 'dears': 3, 'directions': 3, 'cry': 3, 'pattering': 3, 'muttering': 3, 'timid': 3, 'yesterday': 3, 'sorts': 3, 'crossed': 3, 'hoarse': 3, 'spread': 3, 'gently': 3, 'putting': 3, 'guess': 3, 'cause': 3, 'worse': 3, 'slipped': 3, 'Her': 3, 'general': 3, 'WILL': 3, 'O': 3, 'Perhaps': 3, 'notion': 3, 'lesson': 3, 'show': 3, 'nurse': 3, 'dogs': 3, 'dog': 3, 'throw': 3, 'passion': 3, 'Duck': 3, 'Eaglet': 3, 'Caucus': 3, 'fur': 3, 'cross': 3, 'sulky': 3, 'Silence': 3, 'Did': 3, 'crown': 3, 'move': 3, 'pressed': 3, 'prizes': 3, 'chorus': 3, 'voices': 3, 'handed': 3, 'gravely': 3, 'Only': 3, 'speech': 3, 'cheered': 3, 'grave': 3, 'simply': 3, 'solemn': 3, 'noise': 3, 'choked': 3, 'whisper': 3, 'sad': 3, 'tale': 3, 'sighing': 3, 'met': 3, '.--': 3, 'attending': 3, 'pleaded': 3, 'easily': 3, 'joined': 3, 'pity': 3, 'venture': 3, 'carefully': 3, 'remarking': 3, 'suit': 3, 'mentioned': 3, 'Nobody': 3, 'guessed': 3, 'hunting': 3, 'swim': 3, 'mistake': 3, 'plate': 3, 'knocking': 3, 'real': 3, 'Miss': 3, 'floor': 3, 'effect': 3, 'elbow': 3, 'outside': 3, 'stopped': 3, 'forgetting': 3, 'crash': 3, 'Pat': 3, 'sounds': 3, '!--': 3, 'em': 3, 'below': 3, 'THINK': 3, 'kick': 3, 'above': 3, 'sense': 3, 'moving': 3, 'cakes': 3, 'paw': 3, 'hungry': 3, 'picked': 3, 'run': 3, 'edge': 3, 'immediately': 3, 'folded': 3, 'present': 3, 'confusing': 3, 'brought': 3, 'remarks': 3, 'Is': 3, 'Yet': 3, 'Pray': 3, 'stuff': 3, 'free': 3, 'shaking': 3, 'succeeded': 3, 'Serpent': 3, 'serpents': 3, 'girls': 3, 'tasted': 3, 'settled': 3, 'pieces': 3, 'nibbling': 3, 'bringing': 3, 'footman': 3, 'livery': 3, 'considered': 3, 'loudly': 3, 'letter': 3, 'order': 3, 'staring': 3, 'howling': 3, 'instance': 3, 'repeating': 3, 'nursing': 3, 'courage': 3, 'grinned': 3, 'growl': 3, 'faster': 3, 'advantage': 3, 'boy': 3, 'tossing': 3, 'caught': 3, 'shaped': 3, 'proper': 3, 'sobbing': 3, 'less': 3, 'instead': 3, 'Have': 3, 'civil': 3, 'fun': 3, 'Exactly': 3, '\"!\\'': 3, 'breathe': 3, 'o': 3, 'Time': 3, 'HIM': 3, 'cautiously': 3, 'music': 3, 'Twinkle': 3, 'fly': 3, '!\"\\'': 3, 'lady': 3, 'Once': 3, 'lived': 3, 'upset': 3, 'muchness': 3, 'roses': 3, 'beheaded': 3, 'watching': 3, 'checked': 3, 'themselves': 3, 'eager': 3, 'guests': 3, 'AND': 3, 'besides': 3, 'My': 3, 'cards': 3, 'needn': 3, 'Consider': 3, 'Turn': 3, 'unfortunate': 3, 'execution': 3, '?\"\\'': 3, 'hedgehogs': 3, 'tucked': 3, 'turns': 3, 'alive': 3, 'friend': 3, 'anxious': 3, 'mustard': 3, 'Thank': 3, 'MUST': 3, 'Tortoise': 3, 'Drawling': 3, 'week': 3, 'advance': 3, 'figure': 3, 'toes': 3, 'mark': 3, 'snail': 3, 'tails': 3, 'obliged': 3, 'follows': 3, 'around': 3, 'Go': 3, 'Owl': 3, 'Panther': 3, 'pie': 3, 'trumpet': 3, 'spectacles': 3, 'teacup': 3, 'wrote': 3, 'Give': 3, 'list': 3, 'unimportant': 3, 'TO': 3, 'pleasure': 2, 'picking': 2, 'remarkable': 2, 'nor': 2, 'afterwards': 2, 'occurred': 2, ');': 2, 'started': 2, 'hedge': 2, 'straight': 2, 'dipped': 2, 'falling': 2, 'cupboards': 2, 'shelves': 2, 'jar': 2, \"',\": 2, 'tumbling': 2, 'NEVER': 2, 'somewhere': 2, 'thousand': 2, 'learnt': 2, 'showing': 2, 'Latitude': 2, 'Longitude': 2, 'Presently': 2, 'Ma': 2, 'milk': 2, 'earnestly': 2, 'thump': 2, 'wind': 2, 'row': 2, 'doors': 2, 'legged': 2, 'locks': 2, 'longed': 2, 'beds': 2, 'flowers': 2, 'cool': 2, 'fountains': 2, 'shutting': 2, 'label': 2, 'DRINK': 2, 'beautifully': 2, 'wise': 2, 'wild': 2, 'beasts': 2, 'unpleasant': 2, 'friends': 2, 'red': 2, 'burn': 2, 'usually': 2, 'sooner': 2, 'taste': 2, 'mixed': 2, 'brightened': 2, 'lovely': 2, 'crying': 2, 'bring': 2, 'playing': 2, 'forgot': 2, 'bye': 2, '_I_': 2, 'boots': 2, 'sending': 2, 'presents': 2, 'ALICE': 2, 'struck': 2, 'Poor': 2, 'ashamed': 2, 'trotting': 2, 'ringlets': 2, 'knows': 2, 'six': 2, 'twenty': 2, 'Paris': 2, 'Rome': 2, 'doth': 2, 'lap': 2, 'neatly': 2, 'claws': 2, 'smiling': 2, 'jaws': 2, 'rapidly': 2, 'narrow': 2, 'declare': 2, 'bad': 2, 'salt': 2, 'railway': 2, 'conclusion': 2, 'wherever': 2, 'digging': 2, 'splashing': 2, 'Everything': 2, 'swimming': 2, 'wink': 2, 'Conqueror': 2, 'clear': 2, 'ago': 2, 'fright': 2, 'animal': 2, 'feelings': 2, 'quiet': 2, 'purring': 2, 'nicely': 2, 'washing': 2, 'catching': 2, 'brown': 2, 'dinner': 2, 'belongs': 2, 'useful': 2, 'sorrowful': 2, 'hate': 2, 'assembled': 2, 'wet': 2, 'older': 2, 'allow': 2, 'knowing': 2, 'authority': 2, 'ring': 2, 'whose': 2, 'Edwin': 2, 'Morcar': 2, 'earls': 2, 'Mercia': 2, 'Northumbria': 2, 'Ugh': 2, 'proceed': 2, 'archbishop': 2, 'advisable': 2, 'Found': 2, 'frog': 2, 'hurriedly': 2, 'meet': 2, 'offer': 2, 'Speak': 2, 'smile': 2, 'easy': 2, 'hour': 2, 'panting': 2, 'forehead': 2, 'position': 2, 'Prizes': 2, 'comfits': 2, 'absurd': 2, 'caused': 2, 'D': 2, 'Fury': 2, 'law': 2, 'wasting': 2, 'humbly': 2, 'bend': 2, 'knot': 2, 'Crab': 2, 'bird': 2, 'sensation': 2, 'Some': 2, 'throat': 2, 'lonely': 2, 'footsteps': 2, 'ferrets': 2, 'nowhere': 2, 'messages': 2, 'directly': 2, 'ordering': 2, 'drunk': 2, 'stoop': 2, 'curled': 2, 'whatever': 2, 'unhappy': 2, 'fancied': 2, 'woman': 2, 'books': 2, 'trembled': 2, 'proved': 2, 'snatch': 2, 'concluded': 2, 'cucumber': 2, 'Next': 2, 'Digging': 2, 'apples': 2, 'goose': 2, 'TWO': 2, 'bear': 2, 'Mind': 2, \")--'\": 2, 'feeble': 2, 'squeaking': 2, 'comes': 2, 'barrowful': 2, 'shower': 2, 'pebbles': 2, 'rattling': 2, 'hit': 2, 'delighted': 2, 'giving': 2, 'rush': 2, 'safe': 2, 'wandered': 2, 'excellent': 2, 'smallest': 2, 'bark': 2, 'coaxing': 2, 'thistle': 2, 'stretched': 2, 'tiptoe': 2, 'smoking': 2, 'addressed': 2, 'encouraging': 2, 'Explain': 2, 'contemptuously': 2, 'Keep': 2, 'anger': 2, 'unfolded': 2, 'Can': 2, 'LITTLE': 2, 'OLD': 2, 'FATHER': 2, 'WILLIAM': 2, 'Father': 2, 'somersault': 2, 'sell': 2, 'weak': 2, 'father': 2, 'lasted': 2, 'eel': 2, 'clever': 2, 'Be': 2, 'QUITE': 2, 'changing': 2, 'T': 2, 'wretched': 2, 'patiently': 2, 'chose': 2, 'yawned': 2, 'merely': 2, 'taller': 2, 'shorter': 2, 'difficult': 2, 'broke': 2, 'nibbled': 2, 'violent': 2, 'blow': 2, 'lefthand': 2, 'alarm': 2, 'follow': 2, 'distant': 2, 'wandering': 2, 'beating': 2, 'roots': 2, 'doubtfully': 2, 'changes': 2, 'telling': 2, 'matters': 2, 'entangled': 2, 'branches': 2, 'Pig': 2, 'Pepper': 2, 'Fish': 2, 'invitation': 2, 'laughed': 2, 'secondly': 2, 'extraordinary': 2, 'within': 2, 'constant': 2, 'dreadful': 2, 'muttered': 2, 'leaning': 2, 'stirring': 2, 'cauldron': 2, 'pause': 2, 'sneeze': 2, 'introduce': 2, 'throwing': 2, 'plates': 2, 'dishes': 2, 'already': 2, 'takes': 2, 'hint': 2, 'singing': 2, 'line': 2, 'sneezes': 2, 'CHORUS': 2, 'Wow': 2, 'sang': 2, 'thoroughly': 2, 'threw': 2, 'missed': 2, 'twist': 2, 'IF': 2, 'also': 2, 'extremely': 2, 'neither': 2, 'ugly': 2, 'startled': 2, 'explanation': 2, 'denied': 2, 'invited': 2, 'became': 2, 'appear': 2, 'May': 2, 'raving': 2, 'giddy': 2, 'ending': 2, 'raised': 2, 'Suppose': 2, 'front': 2, 'using': 2, 'cushion': 2, 'resting': 2, 'wine': 2, 'laid': 2, 'Your': 2, 'wants': 2, 'personal': 2, 'rude': 2, 'wide': 2, 'SAID': 2, 'riddles': 2, 'break': 2, 'month': 2, 'uneasily': 2, 'BEST': 2, 'meekly': 2, 'cup': 2, 'tells': 2, 'Does': 2, 'year': 2, 'MINE': 2, 'spoon': 2, 'concert': 2, 'Up': 2, 'wash': 2, 'yawning': 2, 'Wake': 2, 'pinched': 2, 'sisters': 2, 'names': 2, 'ill': 2, 'living': 2, 'MORE': 2, 'triumphantly': 2, 'sulkily': 2, 'learning': 2, 'Treacle': 2, 'interrupting': 2, 'rubbing': 2, 'manner': 2, 'closed': 2, 'Really': 2, 'flower': 2, 'painting': 2, 'flat': 2, 'carrying': 2, 'courtiers': 2, 'ornamented': 2, 'royal': 2, 'mostly': 2, 'crimson': 2, 'doubtful': 2, 'lie': 2, 'smiled': 2, 'Get': 2, 'fine': 2, 'Hush': 2, 'scream': 2, 'places': 2, 'flamingoes': 2, 'body': 2, 'unrolled': 2, 'quarrelling': 2, 'stamping': 2, 'shouting': 2, 'dispute': 2, 'removed': 2, 'fight': 2, 'collected': 2, 'unless': 2, 'HER': 2, 'entirely': 2, 'disappeared': 2, 'wildly': 2, 'tempered': 2, 'sugar': 2, 'forget': 2, 'hasn': 2, 'keeping': 2, \"''\": 2, 'Tis': 2, 'tis': 2, 'love': 2, 'Shall': 2, 'experiment': 2, 'bite': 2, 'agree': 2, 'yours': 2, 'choice': 2, 'custody': 2, 'executions': 2, 'sun': 2, 'watched': 2, 'Everybody': 2, 'heart': 2, 'sorrow': 2, 'These': 2, 'heavy': 2, 'proud': 2, 'relief': 2, 'regular': 2, 'Uglification': 2, 'Mystery': 2, 'recovered': 2, 'lobster': 2, 'delightful': 2, 'turtles': 2, 'dancing': 2, 'treading': 2, '?\"': 2, 'kindly': 2, 'thoughts': 2, 'part': 2, 'editions': 2, 'tones': 2, 'persisted': 2, 'cares': 2, 'Pennyworth': 2, 'scroll': 2, 'parchment': 2, 'wig': 2, 'pencil': 2, 'quickly': 2, 'blew': 2, 'blasts': 2, 'summer': 2, 'sent': 2, 'fashion': 2, 'singers': 2, 'deny': 2, 'miserable': 2, 'examine': 2, 'undertone': 2, 'goldfish': 2, 'accident': 2, 'UNimportant': 2, 'note': 2, 'prisoner': 2, 'directed': 2, 'signed': 2, 'atom': 2, 'GAVE': 2, 'THEY': 2, 'FIT': 2, 'wonderful': 2, 'teacups': 2, 'Lewis': 1, 'Carroll': 1, '1865': 1, ']': 1, 'Hole': 1, 'conversations': 1, 'daisy': 1, 'chain': 1, 'daisies': 1, 'pink': 1, 'wondered': 1, 'actually': 1, 'TOOK': 1, 'WATCH': 1, 'OUT': 1, 'ITS': 1, 'WAISTCOAT': 1, 'POCKET': 1, 'flashed': 1, 'waistcoat': 1, 'burning': 1, 'field': 1, 'fortunately': 1, 'pop': 1, 'tunnel': 1, 'stopping': 1, 'Either': 1, 'plenty': 1, 'maps': 1, 'hung': 1, 'pegs': 1, 'labelled': 1, 'ORANGE': 1, 'MARMALADE': 1, 'disappointment': 1, 'empty': 1, 'drop': 1, 'killing': 1, 'brave': 1, 'centre': 1, 'schoolroom': 1, 'practice': 1, 'yes': 1, 'THROUGH': 1, 'downward': 1, 'Antipathies': 1, 'country': 1, 'New': 1, 'Zealand': 1, 'Australia': 1, 'curtsey': 1, 'CURTSEYING': 1, '?)': 1, 'ignorant': 1, 'miss': 1, 'saucer': 1, 'dreamy': 1, 'dozing': 1, 'truth': 1, 'heap': 1, 'sticks': 1, 'overhead': 1, 'hurrying': 1, 'lit': 1, 'lamps': 1, 'locked': 1, 'Suddenly': 1, 'solid': 1, 'belong': 1, 'curtain': 1, 'fifteen': 1, 'lock': 1, 'fitted': 1, 'rat': 1, 'knelt': 1, 'loveliest': 1, 'wander': 1, 'doorway': 1, 'lately': 1, 'telescopes': 1, 'printed': 1, 'letters': 1, 'Drink': 1, 'histories': 1, 'burnt': 1, 'eaten': 1, 'poker': 1, 'bleeds': 1, 'disagree': 1, 'flavour': 1, 'cherry': 1, 'tart': 1, 'custard': 1, 'pine': 1, 'apple': 1, 'roast': 1, 'turkey': 1, 'toffee': 1, 'buttered': 1, 'toast': 1, 'shrink': 1, 'flame': 1, 'blown': 1, 'plainly': 1, 'climb': 1, 'slippery': 1, 'advise': 1, 'advice': 1, 'seldom': 1, 'scolded': 1, 'cheated': 1, 'pretending': 1, 'pretend': 1, 'respectable': 1, 'Soon': 1, 'EAT': 1, 'currants': 1, 'creep': 1, 'ate': 1, \"?',\": 1, 'eats': 1, 'common': 1, 'II': 1, 'Pool': 1, 'Tears': 1, 'Curiouser': 1, 'curiouser': 1, 'largest': 1, 'Good': 1, 'stockings': 1, 'able': 1, ';--': 1, 'Christmas': 1, 'planning': 1, 'carrier': 1, 'odd': 1, 'RIGHT': 1, 'FOOT': 1, 'ESQ': 1, 'HEARTHRUG': 1, 'NEAR': 1, 'FENDER': 1, 'WITH': 1, 'LOVE': 1, 'hopeless': 1, 'Stop': 1, 'shedding': 1, 'gallons': 1, 'reaching': 1, 'dried': 1, 'returning': 1, 'splendidly': 1, 'dressed': 1, 'desperate': 1, 'skurried': 1, 'darkness': 1, 'fanning': 1, 'Dear': 1, 'puzzle': 1, 'Ada': 1, 'Besides': 1, 'five': 1, 'thirteen': 1, 'seven': 1, 'Multiplication': 1, 'Table': 1, 'signify': 1, 'Geography': 1, 'London': 1, 'crocodile': 1, 'Improve': 1, 'shining': 1, 'pour': 1, 'waters': 1, 'Nile': 1, 'scale': 1, 'cheerfully': 1, 'welcome': 1, 'fishes': 1, 'poky': 1, 'toys': 1, 'burst': 1, 'measure': 1, 'avoid': 1, 'existence': 1, 'speed': 1, 'splash': 1, 'somehow': 1, 'seaside': 1, 'coast': 1, 'bathing': 1, 'machines': 1, 'sand': 1, 'wooden': 1, 'spades': 1, 'lodging': 1, 'houses': 1, 'station': 1, 'wept': 1, 'punished': 1, 'drowned': 1, 'walrus': 1, 'hippopotamus': 1, 'harm': 1, 'brother': 1, 'Latin': 1, 'Grammar': 1, \"!')\": 1, 'inquisitively': 1, 'daresay': 1, 'Ou': 1, 'est': 1, 'ma': 1, 'chatte': 1, 'leap': 1, 'quiver': 1, 'passionate': 1, 'soothing': 1, 'lazily': 1, 'sits': 1, 'licking': 1, 'soft': 1, 'bristling': 1, 'Our': 1, 'family': 1, 'HATED': 1, 'nasty': 1, 'vulgar': 1, 'eyed': 1, 'terrier': 1, 'curly': 1, 'farmer': 1, 'hundred': 1, 'pounds': 1, 'kills': 1, 'rats': 1, 'commotion': 1, 'softly': 1, 'III': 1, 'Race': 1, 'Long': 1, 'Tale': 1, 'draggled': 1, 'feathers': 1, 'clinging': 1, 'dripping': 1, 'consultation': 1, 'familiarly': 1, 'known': 1, 'Indeed': 1, 'positively': 1, 'refused': 1, 'Sit': 1, 'LL': 1, 'fixed': 1, 'cold': 1, 'Ahem': 1, 'driest': 1, 'favoured': 1, 'pope': 1, 'submitted': 1, 'leaders': 1, 'accustomed': 1, 'usurpation': 1, 'conquest': 1, 'shiver': 1, 'declared': 1, 'Stigand': 1, 'patriotic': 1, 'Canterbury': 1, 'crossly': 1, 'worm': 1, '\\'\"--': 1, 'Edgar': 1, 'Atheling': 1, 'conduct': 1, 'moderate': 1, 'insolence': 1, 'Normans': 1, 'rising': 1, 'meeting': 1, 'adjourn': 1, 'immediate': 1, 'adoption': 1, 'energetic': 1, 'remedies': 1, 'bent': 1, 'hide': 1, 'tittered': 1, 'audibly': 1, 'paused': 1, 'SOMEBODY': 1, 'inclined': 1, 'winter': 1, 'circle': 1, 'exact': 1, 'shape': 1, 'placed': 1, 'Shakespeare': 1, 'EVERYBODY': 1, 'calling': 1, 'despair': 1, 'pulled': 1, 'luckily': 1, 'prize': 1, 'Hand': 1, 'presented': 1, 'acceptance': 1, 'elegant': 1, 'laugh': 1, 'complained': 1, 'theirs': 1, 'ones': 1, 'patted': 1, 'begged': 1, 'promised': 1, 'C': 1, 'Mine': 1, 'prosecute': 1, 'denial': 1, 'cur': 1, 'Such': 1, 'Sir': 1, 'cunning': 1, 'condemn': 1, 'death': 1, 'fifth': 1, 'undo': 1, 'insult': 1, 'growled': 1, 'quicker': 1, 'daughter': 1, 'lose': 1, 'snappishly': 1, 'patience': 1, 'oyster': 1, 'addressing': 1, 'pet': 1, 'Magpie': 1, 'wrapping': 1, 'Canary': 1, 'bed': 1, 'various': 1, 'pretexts': 1, 'spirited': 1, 'IV': 1, 'Sends': 1, 'Little': 1, 'naturedly': 1, 'completely': 1, 'Run': 1, 'Quick': 1, 'pointed': 1, 'housemaid': 1, 'finds': 1, 'neat': 1, 'brass': 1, 'W': 1, 'RABBIT': 1, 'engraved': 1, 'upstairs': 1, 'lest': 1, 'fancying': 1, 'Coming': 1, 'tidy': 1, 'hoped': 1, 'pairs': 1, 'nevertheless': 1, 'uncorked': 1, 'lips': 1, 'SOMETHING': 1, 'whenever': 1, 'expected': 1, 'pressing': 1, 'ceiling': 1, 'save': 1, 'Alas': 1, 'kneel': 1, 'Still': 1, 'resource': 1, 'Luckily': 1, 'magic': 1, 'grew': 1, 'pleasanter': 1, 'rabbits': 1, 'fairy': 1, 'tales': 1, 'HERE': 1, 'comfort': 1, 'foolish': 1, 'Fetch': 1, 'inwards': 1, 'attempt': 1, 'failure': 1, 'possible': 1, 'frame': 1, 'Sounds': 1, 'pronounced': 1, 'arrum': 1, \".')\": 1, 'fills': 1, 'whispers': 1, 'coward': 1, 'shrieks': 1, 'frames': 1, 'pulling': 1, 'rumbling': 1, 'cartwheels': 1, 'ladder': 1, 'lad': 1, 'tie': 1, 'rope': 1, 'loose': 1, 'Heads': 1, 'Nay': 1, 'Shy': 1, 'fireplace': 1, 'scratching': 1, 'scrambling': 1, 'Catch': 1, 'Brandy': 1, 'choke': 1, 'Last': 1, 'thank': 1, 'ye': 1, 'flustered': 1, 'Jack': 1, 'rocket': 1, 'produced': 1, 'SOME': 1, 'swallowed': 1, 'thick': 1, 'arranged': 1, 'peering': 1, 'enormous': 1, 'feebly': 1, 'stretching': 1, 'touch': 1, 'whistle': 1, 'terribly': 1, 'spite': 1, 'Hardly': 1, 'whereupon': 1, 'yelp': 1, 'rushed': 1, 'worry': 1, 'dodged': 1, 'tumbled': 1, 'heels': 1, 'cart': 1, 'horse': 1, 'trampled': 1, 'series': 1, 'charges': 1, 'forwards': 1, 'barking': 1, 'hoarsely': 1, 'faint': 1, 'leant': 1, 'buttercup': 1, 'fanned': 1, 'teaching': 1, 'tricks': 1, 'blades': 1, 'circumstances': 1, 'caterpillar': 1, 'V': 1, 'Advice': 1, 'languid': 1, 'shyly': 1, 'sternly': 1, 'MYSELF': 1, 'clearly': 1, 'sizes': 1, 'chrysalis': 1, 'butterfly': 1, 'irritated': 1, 'state': 1, 'promising': 1, 'swallowing': 1, 'wait': 1, 'puffed': 1, 'HOW': 1, 'DOTH': 1, 'BUSY': 1, 'BEE': 1, 'Repeat': 1, 'incessantly': 1, 'son': 1, 'feared': 1, 'injure': 1, 'brain': 1, 'uncommonly': 1, 'fat': 1, 'sage': 1, 'grey': 1, 'limbs': 1, 'supple': 1, 'ointment': 1, 'shilling': 1, 'Allow': 1, 'couple': 1, 'tougher': 1, 'suet': 1, 'bones': 1, 'beak': 1, 'argued': 1, 'wife': 1, 'muscular': 1, 'strength': 1, 'jaw': 1, 'Has': 1, 'steady': 1, 'balanced': 1, 'awfully': 1, 'airs': 1, 'altered': 1, 'DON': 1, 'contradicted': 1, 'losing': 1, 'content': 1, 'rearing': 1, 'upright': 1, 'piteous': 1, 'crawled': 1, 'underneath': 1, 'closely': 1, 'swallow': 1, 'morsel': 1, 'immense': 1, 'length': 1, 'rise': 1, 'stalk': 1, 'result': 1, 'curving': 1, 'graceful': 1, 'zigzag': 1, 'dive': 1, 'tops': 1, 'hiss': 1, 'pigeon': 1, 'flown': 1, 'wings': 1, 'subdued': 1, 'sob': 1, 'banks': 1, 'hedges': 1, 'pleasing': 1, 'hatching': 1, 'weeks': 1, 'sorry': 1, 'annoyed': 1, 'highest': 1, 'raising': 1, 'needs': 1, 'wriggling': 1, 'invent': 1, 'deepest': 1, 'contempt': 1, 'denying': 1, 'egg': 1, 'truthful': 1, 'adding': 1, 'YOURS': 1, 'raw': 1, 'nest': 1, 'crouched': 1, 'untwist': 1, 'Whoever': 1, 'frighten': 1, 'wits': 1, 'righthand': 1, 'VI': 1, '--(': 1, 'judging': 1, ')--': 1, 'rapped': 1, 'knuckles': 1, 'footmen': 1, 'powdered': 1, 'crept': 1, 'producing': 1, 'Frog': 1, 'From': 1, 'curls': 1, 'stupidly': 1, 'knocked': 1, 'reasons': 1, 'inside': 1, 'kettle': 1, 'INSIDE': 1, 'knock': 1, 'uncivil': 1, 'tomorrow': 1, 'skimming': 1, 'grazed': 1, 'maybe': 1, 'louder': 1, 'argue': 1, 'drive': 1, 'crazy': 1, 'variations': 1, 'Anything': 1, 'whistling': 1, 'desperately': 1, 'idiotic': 1, 'smoke': 1, 'stool': 1, 'Even': 1, 'sneezed': 1, 'occasionally': 1, 'alternately': 1, 'hearth': 1, 'grinning': 1, 'manners': 1, 'grins': 1, 'violence': 1, 'fix': 1, 'irons': 1, 'saucepans': 1, 'blows': 1, 'PLEASE': 1, 'agony': 1, 'terror': 1, 'PRECIOUS': 1, 'unusually': 1, 'saucepan': 1, 'flew': 1, 'minded': 1, 'axis': 1, 'Talking': 1, 'axes': 1, 'chop': 1, 'glanced': 1, 'Twenty': 1, 'bother': 1, 'abide': 1, 'figures': 1, 'lullaby': 1, 'shake': 1, 'roughly': 1, 'annoy': 1, 'Because': 1, 'teases': 1, '):--': 1, 'howled': 1, 'enjoy': 1, 'pleases': 1, 'flinging': 1, 'frying': 1, 'pan': 1, 'star': 1, 'snorting': 1, 'steam': 1, 'engine': 1, 'doubling': 1, 'straightening': 1, 'tight': 1, 'prevent': 1, 'undoing': 1, 'kill': 1, 'murder': 1, 'grunt': 1, 'expressing': 1, 'snout': 1, 'seriously': 1, 'sobbed': 1, 'NO': 1, 'carry': 1, 'relieved': 1, 'trot': 1, 'handsome': 1, 'seeing': 1, 'bough': 1, 'yards': 1, 'natured': 1, 'teeth': 1, 'treated': 1, 'respect': 1, 'Puss': 1, 'wider': 1, 'depends': 1, 'SOMEWHERE': 1, 'Visit': 1, 'To': 1, 'grant': 1, 'growls': 1, 'wags': 1, 'wag': 1, 'Therefore': 1, 'growling': 1, 'happening': 1, 'hatters': 1, 'branch': 1, 'fig': 1, 'appearing': 1, 'vanishing': 1, 'farther': 1, 'chimneys': 1, 'thatched': 1, 'towards': 1, 'VII': 1, 'Mad': 1, 'Tea': 1, 'Party': 1, 'elbows': 1, 'PLENTY': 1, 'chair': 1, 'cutting': 1, 'severity': 1, 'raven': 1, 'desk': 1, 'ravens': 1, 'desks': 1, 'fourth': 1, 'works': 1, 'grumbled': 1, 'gloomily': 1, 'readily': 1, 'stays': 1, 'poured': 1, 'riddle': 1, 'slightest': 1, 'Nor': 1, 'wearily': 1, 'waste': 1, 'answers': 1, 'accounts': 1, 'terms': 1, 'Half': 1, 'mournfully': 1, 'quarrelled': 1, 'given': 1, 'Like': 1, 'tray': 1, 'pinch': 1, 'bawled': 1, 'murdering': 1, 'mournful': 1, 'whiles': 1, 'vote': 1, 'alarmed': 1, 'proposal': 1, 'fellows': 1, 'quick': 1, 'Elsie': 1, 'Lacie': 1, 'Tillie': 1, 'interest': 1, 'eating': 1, 'drinking': 1, 'ways': 1, 'LESS': 1, 'opinion': 1, 'helped': 1, 'Sh': 1, 'sh': 1, 'interrupt': 1, 'consented': 1, 'promise': 1, 'clean': 1, 'unwillingly': 1, 'jug': 1, 'offend': 1, 'eh': 1, 'IN': 1, 'choosing': 1, 'doze': 1, 'woke': 1, 'traps': 1, 'moon': 1, 'memory': 1, 'drawing': 1, 'rudeness': 1, 'disgust': 1, 'teapot': 1, 'THERE': 1, 'stupidest': 1, 'leading': 1, 'today': 1, 'unlocking': 1, 'THEN': 1, 'VIII': 1, 'Croquet': 1, 'Ground': 1, 'entrance': 1, 'Look': 1, 'paint': 1, 'jogged': 1, 'Always': 1, 'blame': 1, 'deserved': 1, 'spoken': 1, 'tulip': 1, 'onions': 1, 'flung': 1, 'brush': 1, 'unjust': 1, 'chanced': 1, 'RED': 1, 'afore': 1, 'clubs': 1, 'oblong': 1, 'corners': 1, 'diamonds': 1, 'merrily': 1, 'couples': 1, 'hearts': 1, 'Kings': 1, 'Queens': 1, 'recognised': 1, 'noticing': 1, 'velvet': 1, 'KING': 1, 'QUEEN': 1, 'HEARTS': 1, 'processions': 1, 'opposite': 1, 'Idiot': 1, 'THESE': 1, 'rosetree': 1, 'pattern': 1, 'backs': 1, 'fury': 1, 'glaring': 1, 'beast': 1, 'Nonsense': 1, 'bowing': 1, 'Leave': 1, 'humble': 1, 'meanwhile': 1, 'examining': 1, 'remaining': 1, 'execute': 1, 'protection': 1, 'pot': 1, 'marched': 1, 'Their': 1, 'evidently': 1, 'roared': 1, 'peeping': 1, '!\"?\\'': 1, 'boxed': 1, 'laughter': 1, 'hush': 1, 'thunder': 1, 'ridges': 1, 'furrows': 1, 'balls': 1, 'mallets': 1, 'double': 1, 'chief': 1, 'managing': 1, 'comfortably': 1, 'straightened': 1, 'expression': 1, 'bursting': 1, 'laughing': 1, 'provoking': 1, 'act': 1, 'crawling': 1, 'ridge': 1, 'furrow': 1, 'send': 1, 'doubled': 1, 'parts': 1, 'played': 1, 'fighting': 1, 'furious': 1, 'uneasy': 1, 'beheading': 1, 'appearance': 1, 'nodded': 1, 'account': 1, 'someone': 1, 'fairly': 1, 'complaining': 1, 'quarrel': 1, 'oneself': 1, 'attends': 1, 'arch': 1, 'croqueted': 1, 'win': 1, 'finishing': 1, 'kiss': 1, 'likes': 1, 'impertinent': 1, 'king': 1, 'passing': 1, 'settling': 1, 'difficulties': 1, 'screaming': 1, 'search': 1, 'engaged': 1, 'croqueting': 1, 'helpless': 1, 'appealed': 1, 'settle': 1, 'arguments': 1, 'HIS': 1, 'weren': 1, 'prison': 1, 'arrow': 1, 'fading': 1, 'IX': 1, 'Story': 1, 'affectionately': 1, 'pleasant': 1, 'hopeful': 1, 'AT': 1, 'Maybe': 1, 'vinegar': 1, 'sour': 1, 'camomile': 1, 'bitter': 1, 'barley': 1, 'sweet': 1, 'stingy': 1, 'Tut': 1, 'tut': 1, 'squeezed': 1, 'closer': 1, 'uncomfortably': 1, 'bore': 1, 'Somebody': 1, 'minding': 1, 'morals': 1, 'waist': 1, 'Birds': 1, 'feather': 1, 'flock': 1, 'Right': 1, 'mineral': 1, 'attended': 1, 'vegetable': 1, 'imagine': 1, 'cheap': 1, 'birthday': 1, 'Thinking': 1, 'dig': 1, 'worried': 1, 'died': 1, 'favourite': 1, 'linked': 1, 'tremble': 1, 'thunderstorm': 1, 'fair': 1, 'warning': 1, 'absence': 1, 'shade': 1, 'delay': 1, 'cost': 1, 'Those': 1, 'whom': 1, 'sentenced': 1, 'company': 1, 'pardoned': 1, 'picture': 1, 'lazy': 1, 'leaving': 1, 'rubbed': 1, 'chuckled': 1, 'executes': 1, 'ledge': 1, 'rock': 1, 'pitied': 1, 'hollow': 1, 'EVEN': 1, 'occasional': 1, 'exclamation': 1, 'Hjckrrh': 1, 'calmly': 1, 'sink': 1, 'Drive': 1, 'mayn': 1, 'educations': 1, 'VE': 1, 'extras': 1, 'learned': 1, 'Certainly': 1, 'OURS': 1, 'bill': 1, 'WASHING': 1, 'extra': 1, 'afford': 1, 'inquired': 1, 'Reeling': 1, 'Writhing': 1, 'Arithmetic': 1, 'Ambition': 1, 'Distraction': 1, 'Derision': 1, 'lifted': 1, 'uglifying': 1, 'beautify': 1, 'prettier': 1, 'uglify': 1, 'simpleton': 1, 'encouraged': 1, 'counting': 1, 'subjects': 1, 'flappers': 1, 'ancient': 1, 'modern': 1, 'Seaography': 1, 'conger': 1, 'Stretching': 1, 'Fainting': 1, 'Coils': 1, 'stiff': 1, 'Hadn': 1, 'Classics': 1, 'crab': 1, 'Laughing': 1, 'Grief': 1, 'hid': 1, 'Ten': 1, 'lessen': 1, 'eleventh': 1, 'holiday': 1, 'twelfth': 1, 'games': 1, 'X': 1, 'flapper': 1, 'Same': 1, 'bone': 1, 'punching': 1, 'cheeks': 1, 'introduced': 1, \"')\": 1, 'form': 1, 'lines': 1, 'Seals': 1, 'salmon': 1, 'cleared': 1, 'jelly': 1, 'Each': 1, 'partner': 1, 'partners': 1, 'retire': 1, 'bound': 1, 'Swim': 1, 'capering': 1, 'Change': 1, 'yelled': 1, 'Back': 1, 'land': 1, 'dropping': 1, 'pretty': 1, 'forepaws': 1, 'See': 1, 'shingle': 1, 'Too': 1, 'askance': 1, 'thanked': 1, 'scaly': 1, 'England': 1, 'France': 1, 'beloved': 1, 'dinn': 1, 'Dinn': 1, \".--'\": 1, 'thrown': 1, 'DOES': 1, 'BOOTS': 1, 'SHOES': 1, 'shiny': 1, 'blacking': 1, 'Boots': 1, 'Soles': 1, 'eels': 1, 'shrimp': 1, 'anywhere': 1, 'Wouldn': 1, 'journey': 1, 'purpose': 1, '\"?\\'': 1, 'impatient': 1, 'explanations': 1, 'gained': 1, 'listeners': 1, 'Stand': 1, '\"\\'': 1, 'TIS': 1, 'VOICE': 1, 'SLUGGARD': 1, 'baked': 1, 'duck': 1, 'eyelids': 1, 'Trims': 1, 'belt': 1, 'buttons': 1, 'sands': 1, 'gay': 1, 'lark': 1, 'contemptuous': 1, 'Shark': 1, 'tide': 1, 'rises': 1, 'sharks': 1, 'His': 1, 'tremulous': 1, '.]': 1, 'uncommon': 1, 'EVER': 1, 'explained': 1, 'disobey': 1, 'sharing': 1, 'crust': 1, 'gravy': 1, 'meat': 1, 'share': 1, 'treat': 1, 'boon': 1, 'Was': 1, 'permitted': 1, 'received': 1, 'fork': 1, 'banquet': 1, '--]': 1, 'Or': 1, 'Hm': 1, 'accounting': 1, 'tastes': 1, 'Sing': 1, 'rich': 1, 'Waiting': 1, 'tureen': 1, 'dainties': 1, 'Game': 1, 'beauti': 1, 'FUL': 1, 'SOUP': 1, 'Chorus': 1, 'panted': 1, 'faintly': 1, 'breeze': 1, 'XI': 1, 'Stole': 1, 'Tarts': 1, 'seated': 1, 'throne': 1, 'arrived': 1, 'standing': 1, 'chains': 1, 'soldier': 1, 'guard': 1, 'refreshments': 1, 'pass': 1, 'justice': 1, 'wore': 1, 'frontispiece': 1, 'comfortable': 1, 'becoming': 1, 'rightly': 1, 'men': 1, 'Stupid': 1, 'indignant': 1, 'spell': 1, 'neighbour': 1, 'muddle': 1, 'squeaked': 1, 'juror': 1, 'Herald': 1, 'accusation': 1, 'stole': 1, 'Fourteenth': 1, 'Fifteenth': 1, 'Sixteenth': 1, 'Write': 1, 'dates': 1, 'reduced': 1, 'shillings': 1, 'pence': 1, 'hat': 1, 'Stolen': 1, 'memorandum': 1, 'hatter': 1, 'fidgeted': 1, 'spot': 1, 'encourage': 1, 'shifting': 1, 'remain': 1, 'squeeze': 1, 'boldly': 1, 'reasonable': 1, 'pace': 1, 'ridiculous': 1, 'Bring': 1, 'thin': 1, 'dunce': 1, 'twinkled': 1, 'denies': 1, 'speaker': 1, 'canvas': 1, 'bag': 1, 'tied': 1, 'strings': 1, 'newspapers': 1, 'trials': 1, 'attempts': 1, 'applause': 1, 'understood': 1, 'lower': 1, 'SIT': 1, 'officer': 1, 'Shan': 1, 'folding': 1, 'Collar': 1, 'shrieked': 1, 'Behead': 1, 'Suppress': 1, 'Pinch': 1, 'ache': 1, 'fumbled': 1, 'YET': 1, 'Imagine': 1, 'XII': 1, 'Evidence': 1, 'flurry': 1, 'tipped': 1, 'skirt': 1, 'upsetting': 1, 'sprawling': 1, 'reminding': 1, 'globe': 1, 'accidentally': 1, 'BEG': 1, 'dismay': 1, 'vague': 1, 'die': 1, 'cannot': 1, 'emphasis': 1, 'haste': 1, 'downwards': 1, 'unable': 1, 'signifies': 1, 'shock': 1, 'pencils': 1, 'diligently': 1, 'overcome': 1, 'gazing': 1, 'WHATEVER': 1, 'respectful': 1, 'cackled': 1, 'Rule': 1, 'Forty': 1, 'PERSONS': 1, 'THAN': 1, 'MILE': 1, 'HIGH': 1, 'LEAVE': 1, 'COURT': 1, 'mile': 1, 'Nearly': 1, 'invented': 1, 'oldest': 1, 'Number': 1, 'OUTSIDE': 1, 'handwriting': 1, 'queerest': 1, 'imitated': 1, 'prove': 1, 'sign': 1, 'mischief': 1, 'honest': 1, 'clapping': 1, 'PROVES': 1, 'guilt': 1, 'proves': 1, 'Read': 1, 'Begin': 1, 'character': 1, '):': 1, 'push': 1, 'returned': 1, 'Though': 1, 'Involved': 1, 'affair': 1, 'trusts': 1, 'Before': 1, 'fit': 1, 'obstacle': 1, 'Him': 1, 'ourselves': 1, 'secret': 1, 'Between': 1, 'sixpence': 1, 'attempted': 1, 'saves': 1, 'spreading': 1, 'SWIM': 1, 'cardboard': 1, 'WE': 1, 'KNOW': 1, 'BE': 1, 'TRUE': 1, 'RETURNED': 1, 'FROM': 1, 'clearer': 1, 'BEFORE': 1, 'HAD': 1, 'fits': 1, 'furiously': 1, 'inkstand': 1, 'ink': 1, 'trickling': 1, 'pun': 1, 'consider': 1, 'twentieth': 1, 'Sentence': 1, 'Stuff': 1, 'purple': 1, 'flying': 1, 'brushing': 1, 'fluttered': 1, 'kissed': 1, 'setting': 1, 'dreaming': 1, 'dreamed': 1, 'clasped': 1, 'toss': 1, 'listened': 1, 'rustled': 1, 'splashed': 1, 'neighbouring': 1, 'rattle': 1, 'shared': 1, 'meal': 1, 'crashed': 1, 'choking': 1, 'believed': 1, 'reality': 1, 'rustling': 1, 'rippling': 1, 'reeds': 1, 'tinkling': 1, 'sheep': 1, 'bells': 1, 'cries': 1, 'shepherd': 1, 'noises': 1, 'clamour': 1, 'busy': 1, 'farm': 1, 'yard': 1, 'lowing': 1, 'cattle': 1, 'Lastly': 1, 'pictured': 1, 'riper': 1, 'years': 1, 'loving': 1, 'childhood': 1, 'gather': 1, 'THEIR': 1, 'sorrows': 1, 'joys': 1, 'remembering': 1, 'happy': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "alice_freq_list = Counter(alice_words)\n",
    "print(alice_freq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "173\n"
     ]
    }
   ],
   "source": [
    "print(alice_freq_list.get('ALL', 0))\n",
    "print(alice_freq_list.get('All', 0))\n",
    "print(alice_freq_list.get('all', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Computing Frequency List with NLTK\n",
    "NLTK provides `FreqDist` class to construct a Frequency List (`FreqDist` == _Frequency Distribution_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_freq_dist = nltk.FreqDist(alice_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "173\n"
     ]
    }
   ],
   "source": [
    "print(alice_freq_dist.get('ALL', 0))\n",
    "print(alice_freq_dist.get('All', 0))\n",
    "print(alice_freq_dist.get('all', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "- compute frequency list of __lowercased__ \"alice\" corpus (you can use either method)\n",
    "- report `5` most frequent words (use can use provided `nbest` function to get a dict of top N items)\n",
    "- compare the frequencies to the reference values below\n",
    "\n",
    "| Word   | Frequency |\n",
    "|--------|----------:|\n",
    "| ,      |     1,993 |\n",
    "| '      |     1,731 |\n",
    "| the    |     1,642 |\n",
    "| and    |       872 |\n",
    "| .      |       764 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def nbest(d, n=1):\n",
    "    \"\"\"\n",
    "    get n max values from a dict\n",
    "    :param d: input dict (values are numbers, keys are stings)\n",
    "    :param n: number of values to get (int)\n",
    "    :return: dict of top n key-value pairs\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 1993, \"'\": 1731, 'the': 1642, 'and': 872, '.': 764}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_lexicon = set([word.lower() for word in alice_words])\n",
    "\n",
    "alice_lowercase_freq_list = nltk.FreqDist([word.lower() for word in alice_words])# Counter(X) # Replace X with the word list of the corpus in lower case (see above)\n",
    "\n",
    "nbest(alice_lowercase_freq_list, n=1) # Change N from 1 to 5\n",
    "nbest(alice_lowercase_freq_list, n=2) # Change N from 1 to 5\n",
    "nbest(alice_lowercase_freq_list, n=3) # Change N from 1 to 5\n",
    "nbest(alice_lowercase_freq_list, n=4) # Change N from 1 to 5\n",
    "nbest(alice_lowercase_freq_list, n=5) # Change N from 1 to 5\n",
    "# print(alice_lowercase_freq_list.get('queen', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3. Lexicon Operations\n",
    "\n",
    "It is common to process the lexicon according to the task at hand (not every transformation makes sense for all tasks). The common operations are removing words by frequency (minimum or maximum, i.e. *Frequency Cut-Off*) and removing words for a specific lists (i.e. *Stop Word Removal*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.3.1. Frequency Cut-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise 3\n",
    "\n",
    "<!-- - define a function to compute a lexicon from a frequency list applying minimum and maximum frequency cut-offs\n",
    "    \n",
    "    - input: frequence list (dict)\n",
    "    - output: list\n",
    "    - use default values for min and max\n",
    "     -->\n",
    "- Using the function cut_off\n",
    "    \n",
    "    - compute lexicon applying:\n",
    "    \n",
    "        - minimum cut-off 2 (remove words that appear less than 2 times, i.e. remove [hapax legomena](https://en.wikipedia.org/wiki/Hapax_legomenon))\n",
    "        - maximum cut-off 100 (remove words that appear more that 100 times)\n",
    "        - both minimum and maximum thresholds together\n",
    "        \n",
    "    - report size for each comparing to the reference values in the table (on the lowercased lexicon)\n",
    "\n",
    "| Operation  | Min | Max | Size |\n",
    "|------------|----:|----:|-----:|\n",
    "| original   | N/A | N/A | 2636 |\n",
    "| cut-off    |   2 | N/A | 1503 |\n",
    "| cut-off    | N/A | 100 | 2586 |\n",
    "| cut-off    |   2 | 100 | 1453 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 2636\n",
      "CutOFF Min: 10.0 MAX: 500.0  Lexicon Size: 394\n"
     ]
    }
   ],
   "source": [
    "def cut_off(vocab, n_min=100, n_max=100):\n",
    "    new_vocab = []\n",
    "    for word, count in vocab.items():\n",
    "        if count >= n_min and count <= n_max:\n",
    "            new_vocab.append(word)\n",
    "    return new_vocab\n",
    "\n",
    "# lower_bound = float(\"-inf\") # Change these two numbers integer to compute the required cut offs\n",
    "# upper_bound = float(\"inf\")\n",
    "lower_bound = float(10) # Change these two numbers integer to compute the required cut offs\n",
    "upper_bound = float(500)\n",
    "lexicon_cut_off = len(cut_off(alice_lowercase_freq_list, n_min=lower_bound, n_max=upper_bound))\n",
    "\n",
    "print('Original', len(alice_lowercase_freq_list))\n",
    "print('CutOFF Min:', lower_bound, 'MAX:', upper_bound, ' Lexicon Size:', lexicon_cut_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. StopWord Removal\n",
    "\n",
    "In computing, [stop words](https://en.wikipedia.org/wiki/Stop_words) are words filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search.\n",
    "\n",
    "Any group of words can be chosen as the stop words for a given purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's check the stop word lists from the popular python libraries.\n",
    "\n",
    "- spaCy\n",
    "- NLTK\n",
    "- scikit-learn\n",
    "\n",
    "    \n",
    "For NLTK we need to download them first\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy: 326\n",
      "NLTK: 179\n",
      "sklearn: 318\n",
      "{'to', 've', 'weren', 'couldn', 'i', 'and', \"mustn't\", 'aren', 'again', \"doesn't\", 'can', 'were', 'doesn', 'wouldn', 'themselves', \"you'll\", 'about', 'under', 'once', 'down', 'only', 'that', \"she's\", 'isn', 'do', 'but', 'ma', 'have', \"shan't\", 'this', 'shouldn', 'whom', 'any', 'as', 'of', 'why', \"weren't\", 'been', 'm', 'at', \"isn't\", 'her', 'here', \"should've\", 'those', 'too', 'an', 'does', 're', 'won', 'below', 'is', 'are', 'after', 't', 'shan', \"that'll\", 'other', 'through', \"shouldn't\", \"aren't\", 'not', 'who', 'will', 'both', 'should', 'mightn', 'himself', 'in', 'hasn', 'until', 'd', 'be', 'me', 'being', 'more', 'haven', 'yourself', \"you'd\", 'own', 'our', 'am', 'most', 'during', 'so', 'your', 'just', 'before', 'then', 'now', 'll', 'with', 'didn', \"hadn't\", \"you've\", 'his', 'the', 's', 'him', 'has', 'from', \"didn't\", 'myself', 'we', \"won't\", 'you', 'for', \"don't\", 'doing', 'between', 'did', 'yours', 'she', 'up', 'it', 'wasn', 'all', 'mustn', \"wasn't\", 'a', \"hasn't\", 'these', 'each', 'ourselves', 'o', 'don', 'or', 'ain', \"mightn't\", 'itself', 'further', 'needn', \"it's\", 'out', 'which', 'very', 'hers', 'no', 'their', 'on', 'herself', 'while', 'into', 'its', 'than', 'over', 'y', 'if', \"you're\", 'them', 'hadn', 'yourselves', 'they', 'by', 'was', \"needn't\", 'how', 'such', 'my', 'there', 'what', 'he', 'same', 'because', 'had', 'having', 'when', \"couldn't\", \"haven't\", 'few', 'off', \"wouldn't\", 'against', 'theirs', 'some', 'nor', 'ours', 'above', 'where'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yesunerdenejargalsaikhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as SKLEARN_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# nltk.download('stopwords') # Run only once\n",
    "\n",
    "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "print('spaCy: {}'.format(len(SPACY_STOP_WORDS)))\n",
    "print('NLTK: {}'.format(len(NLTK_STOP_WORDS)))\n",
    "print('sklearn: {}'.format(len(SKLEARN_STOP_WORDS)))\n",
    "print(NLTK_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise 4\n",
    "- using Python's built-in `set` [methods](https://docs.python.org/2/library/stdtypes.html#set):\n",
    "    - compute the intersection between the 100 most frequent words in frequency list of the alice corpus and the list of stopwords (report count)\n",
    "    - remove stopwords from the lexicon\n",
    "    - print the size of:\n",
    "            - original lexicon\n",
    "            - lexicon without stopwords\n",
    "            - overlap between 100 most freq. words and stopwords\n",
    "\n",
    "| Operation       | Size |\n",
    "|-----------------|-----:|\n",
    "| original        | 2636 |\n",
    "| no stop words   | 2490 |\n",
    "| top 100 overlap |   65 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b', 'a'}\n",
      "{'d', 'c', 'e'}\n"
     ]
    }
   ],
   "source": [
    "# Set built-in Function\n",
    "set_a = set(['a', 'b', 'c', 'd', 'e'])\n",
    "set_b = set(['a', 'b', 'f'])\n",
    "\n",
    "print(set_a.intersection(set_b)) # Compute overlap\n",
    "print(set_a.difference(set_b)) # Remove Elements by computing the set diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 2636\n",
      "No stopwords 2490\n",
      "To100 overlap 65\n"
     ]
    }
   ],
   "source": [
    "alice_vocab = set([w.lower() for w in alice_words])\n",
    "top100 = list(nbest(alice_lowercase_freq_list,n=100).keys())\n",
    "stop_words = NLTK_STOP_WORDS\n",
    "overlap = stop_words.intersection(top100)# Compute the intersection between top100 and stop_words\n",
    "alice_vocab_no_stopwords = alice_vocab.difference(stop_words)# Remove Stopwords from alice vocab\n",
    "print('Original', len(alice_vocab))\n",
    "print('No stopwords', len(alice_vocab_no_stopwords))\n",
    "print('To100 overlap', len(overlap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Basic Text Pre-processing\n",
    "\n",
    "Both frequency cut-off and stop word removal are frequently used text pre-processing steps. Depending on the application, there are several other common text pre-processing steps that are usually applied for transforming text for Machine Learning tasks.\n",
    "\n",
    "__Text Normalization Steps__\n",
    "\n",
    "- removing extra white spaces\n",
    "\n",
    "- tokenization\n",
    "    - documents to sentences (sentence segmentation/tokenization)\n",
    "    - sentences to tokens\n",
    "\n",
    "- lowercasing/uppercasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- removing punctuation\n",
    "\n",
    "- removing accent marks and other diacritics \n",
    "\n",
    "- removing stop words (see above)\n",
    "\n",
    "- removing sparse terms (frequency cut-off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- number normalization\n",
    "    - numbers to words (i.e. `10` to `ten`)\n",
    "    - number words to numbers (i.e. `ten` to `10`)\n",
    "    - removing numbers\n",
    "\n",
    "- verbalization (specifically for speech applications)\n",
    "\n",
    "    - numbers to words\n",
    "    - expanding abbreviations (or spelling out)\n",
    "    - reading out dates, etc.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation)\n",
    "    - the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "- [stemming](https://en.wikipedia.org/wiki/Stemming)\n",
    "    - the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Tokenization and Sentence Segmentation\n",
    "\n",
    "Given a \"clean\" text, in order to perform any analysis, we need to identify its units.\n",
    "In other words, we need to _segment_ the text into sentences and words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__:\n",
    "Since both _tokenization_ and _sentence segmentation_ are automatic, different tools yield different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Tokenization and Sentence Segmentation with spaCy\n",
    "The default spaCy NLP pipeline does several processing steps including __tokenization__, *part of speech tagging*, lemmatization, *dependency parsing* and *Named Entity Recognition* (we will see the ones in *italics* during the course). \n",
    "\n",
    "\n",
    "SpaCy produces a `Doc` object that contains `Span`s (sentences) and `Token`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "# un-comment the lines above, if you get 'ModuleNotFoundError'\n",
    "nlp = spacy.load(\"en_core_web_sm\",  disable=[\"tagger\", \"ner\"])\n",
    "txt = alice_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "# process the document\n",
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first token: '['\n",
      "first sentence: '[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
      "conversation?'\n",
      "\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "print(\"first token: '{}'\".format(doc[0]))\n",
    "print(\"first sentence: '{}'\".format(list(doc.sents)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37033\n",
      "1558\n"
     ]
    }
   ],
   "source": [
    "# access list of tokens (Token objects)\n",
    "print(len(doc))\n",
    "# access list of sentences (Span objects)\n",
    "print(len(list(doc.sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Tokenization and Sentence Segmentation with NLTK\n",
    "NLTK's [tokenize](https://www.nltk.org/api/nltk.tokenize.html) package provides similar functionality using the methods below.\n",
    "\n",
    "- `word_tokenize` \n",
    "- `sent_tokenize`\n",
    "\n",
    "There are several tokenizer available (read documentation for more information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yesunerdenejargalsaikhan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download NLTK tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33494\n",
      "1625\n"
     ]
    }
   ],
   "source": [
    "alice_words_nltk = nltk.word_tokenize(alice_chars)\n",
    "alice_sents_nltk = nltk.sent_tokenize(alice_chars)\n",
    "print(len(alice_words_nltk))\n",
    "print(len(alice_sents_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first token: '['\n",
      "first sentence: '[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I.'\n"
     ]
    }
   ],
   "source": [
    "print(\"first token: '{}'\".format(alice_words_nltk[0]))\n",
    "print(\"first sentence: '{}'\".format(alice_sents_nltk[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Last Exercise\n",
    "- Load another corpus from Gutenberg (e.g. `milton-paradise.txt`)\n",
    "- On this, compute the descriptive statistics using the provided sentences and tokens (.raw, .words, etc.) as __reference__ \n",
    "    - After this you will get \"reference\" version \n",
    "- Tokenize and segment into sentences the provided raw corpus using the `spaCy` and `NLTK` libraries. Compute the descriptive statistics on the outcome\n",
    "    - After this you will get \"spaCy\" and \"NLTK\" versions\n",
    "- Compute lowercased lexicons for all 3 versions (reference, spaCy, NLTK) of the corpus\n",
    "    - compare lexicon sizes\n",
    "- Compute frequency distribution for all 3 versions (reference, spaCy, NLTK) of the corpus\n",
    "    - compare top N frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
