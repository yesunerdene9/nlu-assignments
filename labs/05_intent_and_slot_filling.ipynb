{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca7d9e5",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "<!-- - pytorch \n",
    "    - Pytorch install: https://pytorch.org/get-started/locally/ \n",
    "- tqdm\n",
    "- sklearn\n",
    "- Huggingface Transformer: \n",
    "    - pip install transformers  -->\n",
    "- **DATASET**:\n",
    "    - https://github.com/BrownFortress/IntentSlotDatasets\n",
    "    - We will use **ATIS** only\n",
    "    \n",
    "    \n",
    "\n",
    "## Outline\n",
    "\n",
    "#### Introduction\n",
    "- sequence labelling (Slot filling)\n",
    "- text classification (Intent classification)\n",
    "\n",
    "#### Preparing text for NN\n",
    "- word2id\n",
    "- special tokens \n",
    "- Customize Dataset class\n",
    "\n",
    "#### Split data in batches\n",
    "- Usage of Dataloader class\n",
    "- Padding sequences\n",
    "\n",
    "#### Neural Networks in Pytorch\n",
    "- Word embeddings\n",
    "- Implementation of an LSTM\n",
    "- Regularization techniques\n",
    "\n",
    "#### Train and Test a Neural Network\n",
    "- Optimizer\n",
    "- Loss function\n",
    "- Iteration over batches\n",
    "\n",
    "#### Hugging face library\n",
    "- Introduction and Usage\n",
    " \n",
    " \n",
    "## References\n",
    "- RNN: https://d2l.ai/chapter_recurrent-neural-networks/index.html \n",
    "- LSTM: https://d2l.ai/chapter_recurrent-modern/lstm.html\n",
    "- GRU: https://d2l.ai/chapter_recurrent-modern/gru.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0b990",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/brownfortress/NLU-2024-labs/blob/main/labs/05_intent_and_slot_filling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b6272",
   "metadata": {},
   "source": [
    "O\n",
    "B \n",
    "I inside\n",
    "IOB tagging\n",
    "\n",
    "\n",
    "2 tag - 2 chunks\n",
    "\n",
    "dont evelauate at token level, but consider the full chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a0554",
   "metadata": {},
   "source": [
    "# 1 Sequence Labeling,  Shallow Parsing and Text classification tasks\n",
    "\n",
    "## 1.1 Sequence Labeling and Shallow parsing\n",
    "Sequence labelling is to assign a label for each token. The task is formally defined as:\n",
    "- Given a sequence of tokens $w = {w_1, w_2, ..., w_n}$,\n",
    "- defining a sequence of labels as $l = {l_1, l_2, ..., l_n}$\n",
    "- compute the sequence $\\hat{l}$ such as $\\hat{l} = \\underset{l}{\\operatorname{argmax}} P(l|w)$ \n",
    "\n",
    "A particular case of sequence labelling is [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing). The main difference from Sequence Labeling task is that Shallow Parsing performs __chunking__ -- segmentation of input sequence into constituents. Chunking is required to identify categories (or types) of *multi-word expressions*.\n",
    "\n",
    "In this, we are going to see a particular case of shallow parsing task, which is named as Slot Filling (or Concept tagging). The **segmentation** part is represented with IOB tags and the **labeling** part are the concepts defined in the annotation schema of a corpus. \\\n",
    "\\\n",
    "An example is the following: \n",
    "\n",
    "| Slot Filling |  |                     |                     |  |  |  |  |\n",
    "|------------------|----|--------------------------|--------------------------|---|------|---|--------|\n",
    "| Input sequence:  | on | april                    | first                    | I | want | a | flight |\n",
    "| Output sequence: | O  | B-depart_date.month_name | B-depart_date.day_number | O | O    | O | O      |\n",
    "\n",
    "## 1.2 Text classification\n",
    "The text classification problem is defined  as follows:\n",
    "- Given a sequence of tokens $w = {w_1, w_2, ..., w_n}$,\n",
    "- And a set of labels $L$ where $l \\in L$\n",
    "- estimate the label $\\hat{l}$ such as $\\hat{l} = \\underset{l}{\\operatorname{argmax}} P(l|w)$ \n",
    "\n",
    "In text classification, the label is given to the whole input sequence instead of at each element of the sequence (as in sequence labelling).\n",
    "\n",
    "The text classification task that we are going to see in this laboratory is named as Intent Classification. The Intent is an additional component of the *semantic frame*. \\\n",
    "\\\n",
    "An example is the following:\n",
    "\n",
    "| Intent Classification|  |                     |                     |  |  |  |  |\n",
    "|------------------|----|--------------------------|--------------------------|---|------|---|--------|\n",
    "| Input sequence:  | on | april                    | first                    | I | want | a | flight |\n",
    "| Output label: | flight     |\n",
    "\n",
    "\n",
    "# 2 Dataset\n",
    "The dataset that we are going to use is ATIS (Airline Travel Information Systems). It is composed of transcriptions of humans asking about flight information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3289e",
   "metadata": {},
   "source": [
    "## 2.2 Load the dataset\n",
    "I have prepared a custom data structure for this dataset. The structure is the following:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "    \"utterance\": \"on april first i need a flight going from phoenix to san diego\", \n",
    "    \"slots\": \"O B-depart_date.month_name B-depart_date.day_number O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name\", \n",
    "    \"intent\": \"flight\"\n",
    "    },\n",
    "    \"...\"\n",
    " ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70494a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "# If you are using Colab, run these commands\n",
    "# !wget -P dataset/ATIS https://raw.githubusercontent.com/BrownFortress/IntentSlotDatasets/main/ATIS/test.json\n",
    "# !wget -P dataset/ATIS https://raw.githubusercontent.com/BrownFortress/IntentSlotDatasets/main/ATIS/train.json\n",
    "# !wget https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/conll.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80808524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "import os\n",
    "device = 'cuda:0' # cuda:0 means we are using the GPU with id 0, if you have multiple GPU\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # Used to report errors on CUDA side\n",
    "PAD_TOKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e1ea7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4978\n",
      "Test samples: 893\n",
      "{'intent': 'flight',\n",
      " 'slots': 'O O O O O B-fromloc.city_name O B-depart_time.time '\n",
      "          'I-depart_time.time O O O B-toloc.city_name O B-arrive_time.time O O '\n",
      "          'B-arrive_time.period_of_day',\n",
      " 'utterance': 'i want to fly from boston at 838 am and arrive in denver at '\n",
      "              '1110 in the morning'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "def load_data(path):\n",
    "    '''\n",
    "        input: path/to/data\n",
    "        output: json \n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(path) as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    return dataset\n",
    "\n",
    "tmp_train_raw = load_data(os.path.join('dataset','ATIS','train.json'))\n",
    "test_raw = load_data(os.path.join('dataset','ATIS','test.json'))\n",
    "print('Train samples:', len(tmp_train_raw))\n",
    "print('Test samples:', len(test_raw))\n",
    "\n",
    "pprint(tmp_train_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b3f37",
   "metadata": {},
   "source": [
    "## 2.3 Create a dev set\n",
    "In the original split the development set (dev set) is missing. To train and find the best hyperparameter of our network the dev set is fundamental. Thus, we have to create it starting from the **traning** set. The dev set is usually the 10% of the dataset. \\\n",
    "Possible sampling strategies:\n",
    "* Take the last n elements of the training set.\n",
    "* Do a random sampling from the training set.\n",
    "* Do a stratified sampling from the training set using one or more criteria. (The best way)\n",
    "    * For further details look [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd6da44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "{'abbreviation': 2.9000000000000004,\n",
      " 'aircraft': 1.6,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airline+flight_no': 0.0,\n",
      " 'airport': 0.4,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.4,\n",
      " 'distance': 0.4,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.4,\n",
      " 'flight_no': 0.2,\n",
      " 'flight_time': 1.0999999999999999,\n",
      " 'ground_fare': 0.4,\n",
      " 'ground_service': 5.1,\n",
      " 'meal': 0.1,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.1}\n",
      "Dev:\n",
      "{'abbreviation': 3.0,\n",
      " 'aircraft': 1.6,\n",
      " 'airfare': 8.4,\n",
      " 'airline': 3.2,\n",
      " 'airport': 0.4,\n",
      " 'capacity': 0.4,\n",
      " 'city': 0.4,\n",
      " 'distance': 0.4,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.4,\n",
      " 'flight_no': 0.2,\n",
      " 'flight_time': 1.0,\n",
      " 'ground_fare': 0.4,\n",
      " 'ground_service': 5.0,\n",
      " 'meal': 0.2,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.2}\n",
      "Test:\n",
      "{'abbreviation': 3.6999999999999997,\n",
      " 'aircraft': 1.0,\n",
      " 'airfare': 5.4,\n",
      " 'airfare+flight': 0.1,\n",
      " 'airline': 4.3,\n",
      " 'airport': 2.0,\n",
      " 'capacity': 2.4,\n",
      " 'city': 0.7000000000000001,\n",
      " 'day_name': 0.2,\n",
      " 'distance': 1.0999999999999999,\n",
      " 'flight': 70.8,\n",
      " 'flight+airfare': 1.3,\n",
      " 'flight+airline': 0.1,\n",
      " 'flight_no': 0.8999999999999999,\n",
      " 'flight_no+airline': 0.1,\n",
      " 'flight_time': 0.1,\n",
      " 'ground_fare': 0.8,\n",
      " 'ground_service': 4.0,\n",
      " 'meal': 0.7000000000000001,\n",
      " 'quantity': 0.3}\n",
      "=========================================================================================\n",
      "TRAIN size: 4480\n",
      "DEV size: 498\n",
      "TEST size: 893\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# First we get the 10% of the training set, then we compute the percentage of these examples \n",
    "\n",
    "portion = 0.10\n",
    "\n",
    "intents = [x['intent'] for x in tmp_train_raw] # We stratify on intents\n",
    "count_y = Counter(intents)\n",
    "\n",
    "labels = []\n",
    "inputs = []\n",
    "mini_train = []\n",
    "\n",
    "for id_y, y in enumerate(intents):\n",
    "    if count_y[y] > 1: # If some intents occurs only once, we put them in training\n",
    "        inputs.append(tmp_train_raw[id_y])\n",
    "        labels.append(y)\n",
    "    else:\n",
    "        mini_train.append(tmp_train_raw[id_y])\n",
    "# Random Stratify\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(inputs, labels, test_size=portion, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=labels)\n",
    "X_train.extend(mini_train)\n",
    "train_raw = X_train\n",
    "dev_raw = X_dev\n",
    "\n",
    "y_test = [x['intent'] for x in test_raw]\n",
    "\n",
    "# Intent distributions\n",
    "print('Train:')\n",
    "pprint({k:round(v/len(y_train),3)*100 for k, v in sorted(Counter(y_train).items())})\n",
    "print('Dev:'), \n",
    "pprint({k:round(v/len(y_dev),3)*100 for k, v in sorted(Counter(y_dev).items())})\n",
    "print('Test:') \n",
    "pprint({k:round(v/len(y_test),3)*100 for k, v in sorted(Counter(y_test).items())})\n",
    "print('='*89)\n",
    "# Dataset size\n",
    "print('TRAIN size:', len(train_raw))\n",
    "print('DEV size:', len(dev_raw))\n",
    "print('TEST size:', len(test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76bbdbe",
   "metadata": {},
   "source": [
    "## 2.3 Convert words to numbers (word2id)\n",
    "Neural Networks in Pytorch, as in other libraries, work with numbers and vectors.\n",
    "<br><br>\n",
    "\n",
    "**Exercise 1** *(10 minutes)*\n",
    "* Create a dictionary that maps the words and labels in the training set to unique  integers $\\geq$ 0, called indexes.\n",
    "    That is:\n",
    "    - One dictionary for mapping words to ids (w2id)\n",
    "    - One dictionary for mapping slot labels to ids (slot2id)\n",
    "    - One dictionary for mapping intent labels to ids (intent2id)\n",
    "\n",
    "* With w2id map the sentence in `sent` into the computed indexes.\n",
    "\n",
    "***Example:***\n",
    "```python\n",
    "dictionary = {\"from\": 2, \"Boston\":88, \"to\":105, \"Tokyo\":42}\n",
    "sent = \"from Boston to Tokyo\" \n",
    "# Output:\n",
    "[2,88,105,42]\n",
    "```\n",
    "\n",
    "We will see later how to convert these indexes into vectors (aka embeddings).\n",
    "\n",
    "*Add special tokens \"pad\" and \"unk\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea03b2aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# for i, word in train_raw:\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     mapping.append({word, i})\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X_train[\u001b[38;5;241m0\u001b[39m: \u001b[38;5;241m5\u001b[39m]):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, utt \u001b[38;5;129;01min\u001b[39;00m (word[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutterance\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     17\u001b[0m         mapping\u001b[38;5;241m.\u001b[39mappend({utt, j})\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m(word[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutterance\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "w2id = {'pad':PAD_TOKEN} # Pad tokens is 0 so the index count should start from 1\n",
    "slot2id = {'pad':PAD_TOKEN} # Pad tokens is 0 so the index count should start from 1\n",
    "intent2id = {}\n",
    "\n",
    "# Map the words only from the train set\n",
    "# Map slot and intent labels of train, dev and test set. 'unk' is not needed.\n",
    "sent = 'I wanna a flight from Toronto to Kuala Lumpur'\n",
    "\n",
    "mapping = [] # convert the sent into indexes using w2id \n",
    "\n",
    "# for i, word in train_raw:\n",
    "#     mapping.append({word, i})\n",
    "for i, word in enumerate(X_train[0: 5]):\n",
    "    for j, utt in (word['utterance']):\n",
    "        mapping.append({utt, j})\n",
    "        print(word['utterance'])\n",
    "\n",
    "print(mapping)\n",
    "\n",
    "print('# Vocab:', len(w2id)-2) # we remove pad and unk from the count\n",
    "print('# Slots:', len(slot2id)-1)\n",
    "print('# Intent:', len(intent2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be408b",
   "metadata": {},
   "source": [
    "## 2.4 Lang class\n",
    "Later we will need to convert those numbers in the original form, so we need to invert those dictionaries. We create a class named as Lang just for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c04f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Lang():\n",
    "    def __init__(self, words, intents, slots, cutoff=0):\n",
    "        self.word2id = self.w2id(words, cutoff=cutoff, unk=True)\n",
    "        self.slot2id = self.lab2id(slots)\n",
    "        self.intent2id = self.lab2id(intents, pad=False)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        self.id2slot = {v:k for k, v in self.slot2id.items()}\n",
    "        self.id2intent = {v:k for k, v in self.intent2id.items()}\n",
    "        \n",
    "    def w2id(self, elements, cutoff=None, unk=True):\n",
    "        vocab = {'pad': PAD_TOKEN}\n",
    "        if unk:\n",
    "            vocab['unk'] = len(vocab)\n",
    "        count = Counter(elements)\n",
    "        for k, v in count.items():\n",
    "            if v > cutoff:\n",
    "                vocab[k] = len(vocab)\n",
    "        return vocab\n",
    "    \n",
    "    def lab2id(self, elements, pad=True):\n",
    "        vocab = {}\n",
    "        if pad:\n",
    "            vocab['pad'] = PAD_TOKEN\n",
    "        for elem in elements:\n",
    "                vocab[elem] = len(vocab)\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d77bc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sum([x['utterance'].split() for x in train_raw], []) # No set() since we want to compute \n",
    "                                                            # the cutoff\n",
    "corpus = train_raw + dev_raw + test_raw # We do not wat unk labels, \n",
    "                                        # however this depends on the research purpose\n",
    "slots = set(sum([line['slots'].split() for line in corpus],[]))\n",
    "intents = set([line['intent'] for line in corpus])\n",
    "\n",
    "lang = Lang(words, intents, slots, cutoff=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac746e",
   "metadata": {},
   "source": [
    "## 2.5 Customize the Dataset class\n",
    "In Pytorch the Dataset class helps you in handeling the dataset. The mandatory methods are ```__init__, __len__ and __getitem__```. <br>\n",
    "You can find more details here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01b4823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class IntentsAndSlots (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, dataset, lang, unk='unk'):\n",
    "        self.utterances = []\n",
    "        self.intents = []\n",
    "        self.slots = []\n",
    "        self.unk = unk\n",
    "        \n",
    "        for x in dataset:\n",
    "            self.utterances.append(x['utterance'])\n",
    "            self.slots.append(x['slots'])\n",
    "            self.intents.append(x['intent'])\n",
    "\n",
    "        self.utt_ids = self.mapping_seq(self.utterances, lang.word2id)\n",
    "        self.slot_ids = self.mapping_seq(self.slots, lang.slot2id)\n",
    "        self.intent_ids = self.mapping_lab(self.intents, lang.intent2id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt = torch.Tensor(self.utt_ids[idx])\n",
    "        slots = torch.Tensor(self.slot_ids[idx])\n",
    "        intent = self.intent_ids[idx]\n",
    "        sample = {'utterance': utt, 'slots': slots, 'intent': intent}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_lab(self, data, mapper):\n",
    "        return [mapper[x] if x in mapper else mapper[self.unk] for x in data]\n",
    "    \n",
    "    def mapping_seq(self, data, mapper): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq.split():\n",
    "                if x in mapper:\n",
    "                    tmp_seq.append(mapper[x])\n",
    "                else:\n",
    "                    tmp_seq.append(mapper[self.unk])\n",
    "            res.append(tmp_seq)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "845ab541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our datasets\n",
    "train_dataset = IntentsAndSlots(train_raw, lang)\n",
    "dev_dataset = IntentsAndSlots(dev_raw, lang)\n",
    "test_dataset = IntentsAndSlots(test_raw, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d297a312",
   "metadata": {},
   "source": [
    "# 3 Batches\n",
    "Batches are used to handle large datasets in the memory. Since the whole dataset cannot fit in GPU memories, we randomly shuffle the dataset and we split it in small batches that will be processed one at a time.\n",
    "## 3.1 Padding\n",
    "Padding is a strategy to fit sequences of different lengths into a matrix. For instance:\n",
    "\n",
    "| Right padding|   |    |   |  |  |  |  \n",
    "|---|----|---|---|---|------|---|\n",
    "| I | saw| a | unk | with | a | telescope | \n",
    "| book | me | a | flight | [pad] | [pad] | [pad] | \n",
    "\n",
    "| Left padding|   |    |   |  |  |  |  \n",
    "|---|----|---|---|---|------|---|\n",
    "| I | saw| a | unk | with | a | telescope | \n",
    "| [pad] | [pad] | [pad] | book | me | a | flight | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874eb4c4",
   "metadata": {},
   "source": [
    "**Exercise 2** *(5 minutes)* <br> \n",
    "Write a function that adds padding on the right. (No need to convert the sentences to numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71bb2fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'saw', 'a', 'man', 'with', 'a', 'telescope']\n",
      "['book', 'me', 'a', 'flight']\n",
      "['I', 'want', 'to', 'see', 'the', 'flights', 'from', 'Milan', 'to', 'Ibiza']\n",
      "[['I', 'saw', 'a', 'man', 'with', 'a', 'telescope'], ['book', 'me', 'a', 'flight'], ['I', 'want', 'to', 'see', 'the', 'flights', 'from', 'Milan', 'to', 'Ibiza']]\n",
      "3\n",
      "6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# split them by white space\n",
    "sequences = ['I saw a man with a telescope', \n",
    "             'book me a flight', \n",
    "             'I want to see the flights from Milan to Ibiza']\n",
    "\n",
    "batches = []\n",
    "for i, sents in enumerate(sequences):\n",
    "    words = sents.split()\n",
    "    batches.append(words)\n",
    "    print(words)\n",
    "\n",
    "print(batches)\n",
    "max_len = max(len(seq) for seq in batches)\n",
    "\n",
    "for i, sent in enumerate(batches):\n",
    "    add_pad_num = max_len - len(sent)\n",
    "    print(add_pad_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b71d2",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "To split the dataset into batches and add padding we will use the DataLoader class. \n",
    "```python\n",
    "DataLoader(Dataset, batch_size=N, collate_fn={custom function}, shuffle=True)\n",
    "```\n",
    "*collate_fn* is used to shape the output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7105180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(data):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(PAD_TOKEN)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        # print(padded_seqs)\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "    data.sort(key=lambda x: len(x['utterance']), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "        \n",
    "    # We just need one length for packed pad seq, since len(utt) == len(slots)\n",
    "    src_utt, _ = merge(new_item['utterance'])\n",
    "    y_slots, y_lengths = merge(new_item[\"slots\"])\n",
    "    intent = torch.LongTensor(new_item[\"intent\"])\n",
    "    \n",
    "    src_utt = src_utt.to(device) # We load the Tensor on our selected device\n",
    "    y_slots = y_slots.to(device)\n",
    "    intent = intent.to(device)\n",
    "    y_lengths = torch.LongTensor(y_lengths).to(device)\n",
    "    \n",
    "    new_item[\"utterances\"] = src_utt\n",
    "    new_item[\"intents\"] = intent\n",
    "    new_item[\"y_slots\"] = y_slots\n",
    "    new_item[\"slots_len\"] = y_lengths\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiations\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn,  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019e479",
   "metadata": {},
   "source": [
    "# 4 Define a neural network in Pytorch\n",
    "In PyTorch the definition of a neural network is quite flexible. In ```__init__``` the layer that is going to be used are instantiated. In ```forward```, the architecture of the neural network is defined. Here you can find all the layers provided by Pytorch https://pytorch.org/docs/stable/nn.html while here you can find the recurrent layers https://pytorch.org/docs/stable/nn.html#recurrent-layers. \n",
    "\n",
    "<br><br>\n",
    "**pack_padded_sequence** and **pad_packed_sequences** respectively compress and uncompress sequences to remove the padding embeddings from the computation, reducing the computational cost and, therefore, the CO2 emissions.\n",
    " ![](https://i.stack.imgur.com/LPHAs.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93adc878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class ModelIAS(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=1, pad_index=0):\n",
    "        super(ModelIAS, self).__init__()\n",
    "        # hid_size = Hidden size\n",
    "        # out_slot = number of slots (output size for slot filling)\n",
    "        # out_int = number of intents (output size for intent class)\n",
    "        # emb_size = word embedding size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
    "        \n",
    "        self.utt_encoder = nn.LSTM(emb_size, hid_size, n_layer, bidirectional=False, batch_first=True)    \n",
    "        self.slot_out = nn.Linear(hid_size, out_slot)\n",
    "        self.intent_out = nn.Linear(hid_size, out_int) # why 2 linear layer -> 2 different task slot and int\n",
    "        # Dropout layer How/Where do we apply it?\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, utterance, seq_lengths):\n",
    "        # utterance.size() = batch_size X seq_len\n",
    "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
    "        \n",
    "        # pack_padded_sequence avoid computation over pad tokens reducing the computational cost\n",
    "        \n",
    "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy(), batch_first=True)\n",
    "        # Process the batch\n",
    "        packed_output, (last_hidden, cell) = self.utt_encoder(packed_input) \n",
    "       \n",
    "        # Unpack the sequence\n",
    "        utt_encoded, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        # Get the last hidden state\n",
    "        last_hidden = last_hidden[-1,:,:]\n",
    "        \n",
    "        # Is this another possible way to get the last hiddent state? (Why?)\n",
    "        # utt_encoded.permute(1,0,2)[-1]\n",
    "        \n",
    "        # Compute slot logits\n",
    "        slots = self.slot_out(utt_encoded)\n",
    "        # Compute intent logits\n",
    "        intent = self.intent_out(last_hidden)\n",
    "        \n",
    "        # Slot size: batch_size, seq_len, classes \n",
    "        slots = slots.permute(0,2,1) # We need this for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        return slots, intent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992a22c",
   "metadata": {},
   "source": [
    "## 3.1 Function to randomly initialize the weights\n",
    "This is a generic function that randomly initialize the parameters of RNN networks and linear layers. To dig deep in to this I would suggest you to look at here: https://pytorch.org/docs/master/nn.init.html \\\n",
    "\\\n",
    "*Note: In Pytorch every parameter of the network has a proper name like weight_ih, weight_hh etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f47fe3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a362fc",
   "metadata": {},
   "source": [
    "## 3.2 Training set up\n",
    "We initialize the model and we select the hyperparameters of the neural network. Futhermore, we initialize the optimizer and we select the loss function.\n",
    "- You can find further optimization algorithms here: https://pytorch.org/docs/stable/optim.html\n",
    "- and further loss functions here: https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5edf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = ModelIAS(hid_size, out_slot, out_int, emb_size, vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a18f4",
   "metadata": {},
   "source": [
    "### Train Loop and Evaluation Loop\n",
    "We define two functions one for training our model and the other for evaluating it. To compute the performances on the slot filling task we will use the **conll script**, while for the intent classification task we are going to use the **classification_report**.\n",
    "\n",
    "<br>\n",
    "\n",
    "In the literature, the Intent Classification task is evaluated using accuracy as a metric. The Slot filling task is evaluated using the *conll script* which computes the performance at the chunk level and the F1 score is usually reported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6bf6dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conll import evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_loop(data, optimizer, criterion_slots, criterion_intents, model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        slots, intent = model(sample['utterances'], sample['slots_len'])\n",
    "        loss_intent = criterion_intents(intent, sample['intents'])\n",
    "        loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "        loss = loss_intent + loss_slot # In joint training we sum the losses. \n",
    "                                       # Is there another way to do that?\n",
    "        loss_array.append(loss.item())\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "    return loss_array\n",
    "\n",
    "def eval_loop(data, criterion_slots, criterion_intents, model, lang):\n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    \n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "    \n",
    "    ref_slots = []\n",
    "    hyp_slots = []\n",
    "    #softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            slots, intents = model(sample['utterances'], sample['slots_len'])\n",
    "            loss_intent = criterion_intents(intents, sample['intents'])\n",
    "            loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "            loss = loss_intent + loss_slot \n",
    "            loss_array.append(loss.item())\n",
    "            # Intent inference\n",
    "            # Get the highest probable class\n",
    "            out_intents = [lang.id2intent[x] \n",
    "                           for x in torch.argmax(intents, dim=1).tolist()] \n",
    "            gt_intents = [lang.id2intent[x] for x in sample['intents'].tolist()]\n",
    "            ref_intents.extend(gt_intents)\n",
    "            hyp_intents.extend(out_intents)\n",
    "            \n",
    "            # Slot inference \n",
    "            output_slots = torch.argmax(slots, dim=1)\n",
    "            for id_seq, seq in enumerate(output_slots):\n",
    "                length = sample['slots_len'].tolist()[id_seq]\n",
    "                utt_ids = sample['utterance'][id_seq][:length].tolist()\n",
    "                gt_ids = sample['y_slots'][id_seq].tolist()\n",
    "                gt_slots = [lang.id2slot[elem] for elem in gt_ids[:length]]\n",
    "                utterance = [lang.id2word[elem] for elem in utt_ids]\n",
    "                to_decode = seq[:length].tolist()\n",
    "                ref_slots.append([(utterance[id_el], elem) for id_el, elem in enumerate(gt_slots)])\n",
    "                tmp_seq = []\n",
    "                for id_el, elem in enumerate(to_decode):\n",
    "                    tmp_seq.append((utterance[id_el], lang.id2slot[elem]))\n",
    "                hyp_slots.append(tmp_seq)\n",
    "    try:            \n",
    "        results = evaluate(ref_slots, hyp_slots)\n",
    "    except Exception as ex:\n",
    "        # Sometimes the model predicts a class that is not in REF\n",
    "        print(\"Warning:\", ex)\n",
    "        ref_s = set([x[1] for x in ref_slots])\n",
    "        hyp_s = set([x[1] for x in hyp_slots])\n",
    "        print(hyp_s.difference(ref_s))\n",
    "        results = {\"total\":{\"f\":0}}\n",
    "        \n",
    "    report_intent = classification_report(ref_intents, hyp_intents, \n",
    "                                          zero_division=False, output_dict=True)\n",
    "    return results, report_intent, loss_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2eaee",
   "metadata": {},
   "source": [
    "## 3.3 Train a neural network\n",
    "We train a neural network iterating several times over the training set. \n",
    "* **epochs**: number of times in which the whole training set is seen by the network\n",
    "* **early stopping**: keeps controlled the performance of the model on the dev set and interrupts the training when the performance is getting worse\n",
    "    * **patience**: wait for a number of step before interrupting the training, even though the performance is getting worse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "n_epochs = 200\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "for x in tqdm(range(1,n_epochs)):\n",
    "    loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                      criterion_intents, model, clip=clip)\n",
    "    if x % 5 == 0: # We check the performance every 5 epochs\n",
    "        sampled_epochs.append(x)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                      criterion_intents, model, lang)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        \n",
    "        f1 = results_dev['total']['f']\n",
    "        # For decreasing the patience you can also use the average between slot f1 and intent accuracy\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            # Here you should save the model\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "\n",
    "results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                         criterion_intents, model, lang)    \n",
    "print('Slot F1: ', results_test['total']['f'])\n",
    "print('Intent Accuracy:', intent_test['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc39e9c",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "To save the model you have to save:\n",
    "- The weights of the model\n",
    "- The computed vocabularies (w2id, slot2id, intent2id)\n",
    "- The optimizer (optionally, only if you want to continue with the training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = os.path.join(\"bin\", model_name)\n",
    "# saving_object = {\"epoch\": x, \n",
    "#                  \"model\": model.state_dict(), \n",
    "#                  \"optimizer\": optimizer.state_dict(), \n",
    "#                  \"w2id\": w2id, \n",
    "#                  \"slot2id\": slot2id, \n",
    "#                  \"intent2id\": intent2id}\n",
    "# torch.save(saving_object, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1466a",
   "metadata": {},
   "source": [
    "### Plot of the train and valid losses\n",
    "One of the techniques for debugging a neural network is to check the plot of the loss. If the loss goes smoothly down then the network works correctly, otherwise a deeper analysis is needed. Furthermore, this plot can be useful for deciding the learning rate and the optimizer algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1211aab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num = 3, figsize=(8, 5)).patch.set_facecolor('white')\n",
    "plt.title('Train and Dev Losses')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.plot(sampled_epochs, losses_train, label='Train loss')\n",
    "plt.plot(sampled_epochs, losses_dev, label='Dev loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420df73c",
   "metadata": {},
   "source": [
    "### Multiple runs\n",
    "To have reliable results on small corpora we have to train and test the model from scratch for several times. At the end, we average the results and we compute the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "n_epochs = 200\n",
    "runs = 5\n",
    "\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    model = ModelIAS(hid_size, out_slot, out_int, emb_size, \n",
    "                     vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    criterion_intents = nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    patience = 3\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "    for x in range(1,n_epochs):\n",
    "        loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                          criterion_intents, model)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                          criterion_intents, model, lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stopping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                             criterion_intents, model, lang)\n",
    "    intent_acc.append(intent_test['accuracy'])\n",
    "    slot_f1s.append(results_test['total']['f'])\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4565d",
   "metadata": {},
   "source": [
    " ![](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\n",
    "# Hugging Face\n",
    "Hugging Face is a library that allows you to use pretrained models in an easy way. This means that you do not need to implement an architecture and train it from scratch. Hugging Face is based on a community where people share trained models and code.\n",
    "<br/><br/>\n",
    "In Hugging Face there are many different models (https://huggingface.co/models) that you can import and each of them has its own input and output shapes. However, Transformer-based models are usually composed of two parts: \n",
    "- **Tokenizer**\n",
    "- **Architecture/Pretrained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT model script from: huggingface.co\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # Download the tokenizer\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\") # Download the model\n",
    "\n",
    "inputs = tokenizer([\"I saw a man with a telescope\", \"StarLord was here\",  \"I didn't\"], return_tensors=\"pt\", padding=True)\n",
    "pprint(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f26671",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9139e1c",
   "metadata": {},
   "source": [
    "##  Byte pair encoding\n",
    "The tricky part of using Transformer-based model is the tokenizer which is based on byte pair encoding algorithm. Indeed, the **tokenizers** used by Transformer-based models are different from those we have seen in the lab. While for instance Spacy's tokenizer is rule-based and splits the text looking at the punctuation, the goal of Transformer tokenizers is to reduce the vocabulary length by splitting words into subwords. A thoroughly explanation of such these tokenizers can be found here: https://huggingface.co/docs/transformers/tokenizer_summary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60584d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[\"input_ids\"][0])\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2dcc1f",
   "metadata": {},
   "source": [
    "# Mandatory Exam Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2fad3c",
   "metadata": {},
   "source": [
    "## Part 1 (4 points)\n",
    "As for LM project, you have to apply these two modifications incrementally. Also in this case you may have to play with the hyperparameters and optimizers to improve the performance. \n",
    "\n",
    "Modify the baseline architecture Model IAS by:\n",
    "- Adding bidirectionality\n",
    "- Adding dropout layer\n",
    "\n",
    "**Intent classification**: accuracy <br>\n",
    "**Slot filling**: F1 score with conll\n",
    "\n",
    "***Dataset to use: ATIS***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30eb2ce-9952-4127-b258-3d372f452c2b",
   "metadata": {},
   "source": [
    "## Part 2 (11 points)\n",
    "\n",
    "Adapt the code to fine-tune a pre-trained BERT model using a multi-task learning setting on intent classification and slot filling. \n",
    "You can refer to this paper to have a better understanding of how to implement this: https://arxiv.org/abs/1902.10909. In this, one of the challenges of this is to handle the sub-tokenization issue.\n",
    "\n",
    "*Note*: The fine-tuning process is to further train on a specific task/s a model that has been pre-trained on a different (potentially unrelated) task/s.\n",
    "\n",
    "\n",
    "The models that you can experiment with are [*BERT-base* or *BERT-large*](https://huggingface.co/google-bert/bert-base-uncased). \n",
    "\n",
    "**Intent classification**: accuracy <br>\n",
    "**Slot filling**: F1 score with conll\n",
    "\n",
    "***Dataset to use: ATIS***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b030c94",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
